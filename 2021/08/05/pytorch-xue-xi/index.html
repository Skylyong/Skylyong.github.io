<!DOCTYPE html>
<html lang="zh-CN">
    <!-- title -->
<!-- keywords -->
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="author" content="ğŸ†">
    <meta name="renderer" content="webkit">
    <meta name="copyright" content="ğŸ†">
        <meta name="keywords" content="hexo,hexo-theme,hexo-blog">
    <meta name="description" content="Keep go on">
    <meta name="description" content="è¯¾ç¨‹[é“¾æ¥](https: &#x2F;&#x2F; www.youtube.com &#x2F; watch?v &#x3D; c36lUUr864M) 1. Tensor1.1 How to create tensor æ„é€ tensorçš„æ–¹æ³•ä¸»è¦æœ‰å…­ç§ï¼Œåˆ†åˆ«å¦‚ä¸‹ï¼š  x &#x3D; torch.ones(3, 5, dtype&#x3D;torch.float, requires_grad &#x3D; True )  x &#x3D; torch.zeros(3,">
<meta property="og:type" content="article">
<meta property="og:title" content="Pytorchå­¦ä¹ ">
<meta property="og:url" content="https://ailyong.cn/2021/08/05/pytorch-xue-xi/index.html">
<meta property="og:site_name" content="Lyong&#39;s blog">
<meta property="og:description" content="è¯¾ç¨‹[é“¾æ¥](https: &#x2F;&#x2F; www.youtube.com &#x2F; watch?v &#x3D; c36lUUr864M) 1. Tensor1.1 How to create tensor æ„é€ tensorçš„æ–¹æ³•ä¸»è¦æœ‰å…­ç§ï¼Œåˆ†åˆ«å¦‚ä¸‹ï¼š  x &#x3D; torch.ones(3, 5, dtype&#x3D;torch.float, requires_grad &#x3D; True )  x &#x3D; torch.zeros(3,">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.githubusercontent.com/Skylyong/i/main/20210805210347.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Skylyong/i/main/20210805213343.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Skylyong/i/main/20210805230250.png">
<meta property="article:published_time" content="2021-08-05T13:50:37.000Z">
<meta property="article:modified_time" content="2021-08-05T15:04:48.000Z">
<meta property="article:author" content="ğŸ†">
<meta property="article:tag" content="pytorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/Skylyong/i/main/20210805210347.png">
    <meta http-equiv="Cache-control" content="no-cache">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <link rel="icon" href="/assets/favicon.ico">
    <title>Pytorchå­¦ä¹  Â· èŒ„å­çš„ä¸ªäººç©ºé—´</title>
    <!-- /*! loadCSS. [c]2017 Filament Group, Inc. MIT License */
/* This file is meant as a standalone workflow for
- testing support for link[rel=preload]
- enabling async CSS loading in browsers that do not support rel=preload
- applying rel preload css once loaded, whether supported or not.
*/ -->
<script>
    (function (w) {
        'use strict'
        // rel=preload support test
        if (!w.loadCSS) {
            w.loadCSS = function () {}
        }
        // define on the loadCSS obj
        var rp = (loadCSS.relpreload = {})
        // rel=preload feature support test
        // runs once and returns a function for compat purposes
        rp.support = (function () {
            var ret
            try {
                ret = w.document.createElement('link').relList.supports('preload')
            } catch (e) {
                ret = false
            }
            return function () {
                return ret
            }
        })()

        // if preload isn't supported, get an asynchronous load by using a non-matching media attribute
        // then change that media back to its intended value on load
        rp.bindMediaToggle = function (link) {
            // remember existing media attr for ultimate state, or default to 'all'
            var finalMedia = link.media || 'all'

            function enableStylesheet() {
                link.media = finalMedia
            }

            // bind load handlers to enable media
            if (link.addEventListener) {
                link.addEventListener('load', enableStylesheet)
            } else if (link.attachEvent) {
                link.attachEvent('onload', enableStylesheet)
            }

            // Set rel and non-applicable media type to start an async request
            // note: timeout allows this to happen async to let rendering continue in IE
            setTimeout(function () {
                link.rel = 'stylesheet'
                link.media = 'only x'
            })
            // also enable media after 3 seconds,
            // which will catch very old browsers (android 2.x, old firefox) that don't support onload on link
            setTimeout(enableStylesheet, 3000)
        }

        // loop through link elements in DOM
        rp.poly = function () {
            // double check this to prevent external calls from running
            if (rp.support()) {
                return
            }
            var links = w.document.getElementsByTagName('link')
            for (var i = 0; i < links.length; i++) {
                var link = links[i]
                // qualify links to those with rel=preload and as=style attrs
                if (
                    link.rel === 'preload' &&
                    link.getAttribute('as') === 'style' &&
                    !link.getAttribute('data-loadcss')
                ) {
                    // prevent rerunning on link
                    link.setAttribute('data-loadcss', true)
                    // bind listeners to toggle media back
                    rp.bindMediaToggle(link)
                }
            }
        }

        // if unsupported, run the polyfill
        if (!rp.support()) {
            // run once at least
            rp.poly()

            // rerun poly on an interval until onload
            var run = w.setInterval(rp.poly, 500)
            if (w.addEventListener) {
                w.addEventListener('load', function () {
                    rp.poly()
                    w.clearInterval(run)
                })
            } else if (w.attachEvent) {
                w.attachEvent('onload', function () {
                    rp.poly()
                    w.clearInterval(run)
                })
            }
        }

        // commonjs
        if (typeof exports !== 'undefined') {
            exports.loadCSS = loadCSS
        } else {
            w.loadCSS = loadCSS
        }
    })(typeof global !== 'undefined' ? global : this)
</script>

    <style type="text/css">
    @font-face {
        font-family: 'Oswald-Regular';
        src: url("/font/Oswald-Regular.ttf");
    }

    body {
        margin: 0;
    }

    header,
    footer,
    .footer-fixed-btn,
    .sidebar,
    .container,
    .site-intro-meta,
    .toc-wrapper {
        display: none;
    }

    .site-intro {
        position: relative;
        z-index: 3;
        width: 100%;
        /* height: 50vh; */
        overflow: hidden;
    }

    .site-intro-placeholder {
        position: absolute;
        z-index: -2;
        top: 0;
        left: 0;
        width: calc(100% + 300px);
        height: 100%;
        background: repeating-linear-gradient(
            -45deg,
            #444 0,
            #444 80px,
            #333 80px,
            #333 160px
        );
        background-position: center center;
        transform: translate3d(-226px, 0, 0);
        animation: gradient-move 2.5s ease-out 0s infinite;
    }

    @keyframes gradient-move {
        0% {
            transform: translate3d(-226px, 0, 0);
        }
        100% {
            transform: translate3d(0, 0, 0);
        }
    }
</style>

    <link id="stylesheet-fancybox" rel="preload" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.36/dist/fancybox/fancybox.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
    <link id="stylesheet-base" rel="preload" href="/css/style.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
    <link id="stylesheet-mobile" rel="preload" href="/css/mobile.css" as="style" onload="this.onload=null;this.rel='stylesheet';this.media='screen and (max-width: 960px)'">
    <link id="stylesheet-theme-dark" rel="preload" href="/css/dark.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" as="script">
    <link rel="preload" href="/scripts/main.js" as="script">
    <link rel="preload" href="/font/Oswald-Regular.ttf" as="font" crossorigin>
    <link rel="preload" href="https://at.alicdn.com/t/font_327081_1dta1rlogw17zaor.woff" as="font" crossorigin>
    <!-- algolia -->
    <!-- ç™¾åº¦ç»Ÿè®¡  -->
    <!-- è°·æ­Œç»Ÿè®¡  -->
    <!-- Google tag (gtag.js) -->
<meta name="generator" content="Hexo 5.4.2"><link rel="alternate" href="/atom.xml" title="Lyong's blog" type="application/atom+xml">
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js"></script>
    <script type="text/javascript">
        if (typeof window.$ == undefined) {
            console.warn('jquery load from jsdelivr failed, will load local script')
            document.write('<script src="/lib/jquery.min.js" />')
        }
    </script>
        <body class="post-body">
        <!-- header -->
        <header class="header header-mobile">
    <!-- top read progress line -->
    <div class="header-element">
        <div class="read-progress"></div>
    </div>
    <!-- sidebar menu button -->
    <div class="header-element">
        <div class="header-sidebar-menu">
            <div style="padding-left: 1px;">&#xe775;</div>
        </div>
    </div>
    <!-- header actions -->
    <div class="header-actions">
        <!-- theme mode switch button -->
        <span class="header-theme-btn header-element">
            <i class="fas fa-adjust"></i>
        </span>
        <!-- back to home page text -->
        <span class="home-link header-element">
            <a href="/">èŒ„å­çš„ä¸ªäººç©ºé—´</a>
        </span>
    </div>
    <!-- toggle banner -->
    <div class="banner">
        <div class="blog-title header-element">
            <a href="/">èŒ„å­çš„ä¸ªäººç©ºé—´</a>
        </div>
        <div class="post-title header-element">
            <a href="#" class="post-name">Pytorchå­¦ä¹ </a>
        </div>
    </div>
</header>

        <!-- fixed footer -->
        <footer class="footer-fixed">
    <!-- donate button -->
    <div class="donate-popup donate-popup--hidden">
    <div class="donate-popup__title">Buy Me A Coffee</div>
    <div class="donate-popup__content">
        <div class="donate-popup__content-description">è°¢è°¢ä½ çš„é¼“åŠ±å’Œæ”¯æŒï¼</div>
        <img
            class="donate-popup__content-qrCode"
            title="Wechat"
            alt="Wechat"
            src="/assets/donate-wechat.png"
        ></img>
        <img
            class="donate-popup__content-qrCode"
            title="Alipay"
            alt="Alipay"
            src="/assets/donate-alipay.jpg"
        ></img>
    </div>
</div>

    <div
        title="Donate to the author"
        class="footer-fixed-btn footer-fixed-btn--hidden donate-btn"
    >
        <i class="fas fa-donate"></i>
    </div>

    <!-- back to top button -->
    <div class="footer-fixed-btn footer-fixed-btn--hidden back-top">
        <div>&#xe639;</div>
    </div>
</footer>

        <!-- wrapper -->
        <div class="wrapper">
            <div class="site-intro" style="    height:50vh;
">
    <!-- ä¸»é¡µ  -->
    <!-- 404é¡µ  -->
    <div class="site-intro-placeholder"></div>
    <div class="site-intro-img" style="background-image: url(/intro/post-bg.jpg)"></div>
    <div class="site-intro-meta">
        <!-- æ ‡é¢˜  -->
        <h1 class="intro-title">
            <!-- ä¸»é¡µ  -->
                Pytorchå­¦ä¹ 
            <!-- 404 -->
        </h1>
        <!-- å‰¯æ ‡é¢˜ -->
        <p class="intro-subtitle">
            <!-- ä¸»é¡µå‰¯æ ‡é¢˜  -->
            <!-- 404 -->
        </p>
        <!-- æ–‡ç« é¡µ meta -->
            <div class="post-intros">
                <!-- æ–‡ç« é¡µæ ‡ç­¾  -->
                    <div class="post-intro-tags" >
        <a class="post-tag" href="javascript:void(0);" data-tags="pytorch">pytorch</a>
</div>

                <!-- æ–‡ç« å­—æ•°ç»Ÿè®¡ -->
                    <div class="post-intro-read">
                        <span>å­—æ•°ç»Ÿè®¡: <span class="post-count word-count">7.1k</span>é˜…è¯»æ—¶é•¿: <span class="post-count reading-time">40 min</span></span>
                    </div>
                <div class="post-intro-meta">
                    <!-- æ’°å†™æ—¥æœŸ -->
                    <span class="iconfont-archer post-intro-calander">&#xe676;</span>
                    <span class="post-intro-time">2021/08/05</span>
                    <!-- busuanzi -->
                        <span id="busuanzi_container_page_pv" class="busuanzi-pv">
                            <span class="iconfont-archer post-intro-busuanzi">&#xe602;</span>
                            <span id="busuanzi_value_page_pv"></span>
                        </span>
                    <!-- æ–‡ç« åˆ†äº« -->
                    <span class="share-wrapper">
                        <span class="iconfont-archer share-icon">&#xe71d;</span>
                        <span class="share-text">Share</span>
                        <ul class="share-list">
                            <li class="iconfont-archer share-qr" data-type="qr">&#xe75b;
                                <div class="share-qrcode"></div>
                            </li>
                            <li class="iconfont-archer" data-type="weibo">&#xe619;</li>
                            <li class="iconfont-archer" data-type="qzone">&#xe62e;</li>
                            <li class="iconfont-archer" data-type="twitter">&#xe634;</li>
                            <li class="iconfont-archer" data-type="facebook">&#xe67a;</li>
                        </ul>
                    </span>
                </div>
            </div>
    </div>
</div>

            <script>
  // get user agent
  function getBrowserVersions() {
    var u = window.navigator.userAgent
    return {
      userAgent: u,
      trident: u.indexOf('Trident') > -1, //IEå†…æ ¸
      presto: u.indexOf('Presto') > -1, //operaå†…æ ¸
      webKit: u.indexOf('AppleWebKit') > -1, //è‹¹æœã€è°·æ­Œå†…æ ¸
      gecko: u.indexOf('Gecko') > -1 && u.indexOf('KHTML') == -1, //ç«ç‹å†…æ ¸
      mobile: !!u.match(/AppleWebKit.*Mobile.*/), //æ˜¯å¦ä¸ºç§»åŠ¨ç»ˆç«¯
      ios: !!u.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/), //iosç»ˆç«¯
      android: u.indexOf('Android') > -1 || u.indexOf('Linux') > -1, //androidç»ˆç«¯æˆ–è€…ucæµè§ˆå™¨
      iPhone: u.indexOf('iPhone') > -1 || u.indexOf('Mac') > -1, //æ˜¯å¦ä¸ºiPhoneæˆ–è€…å®‰å“QQæµè§ˆå™¨
      iPad: u.indexOf('iPad') > -1, //æ˜¯å¦ä¸ºiPad
      webApp: u.indexOf('Safari') == -1, //æ˜¯å¦ä¸ºwebåº”ç”¨ç¨‹åºï¼Œæ²¡æœ‰å¤´éƒ¨ä¸åº•éƒ¨
      weixin: u.indexOf('MicroMessenger') == -1, //æ˜¯å¦ä¸ºå¾®ä¿¡æµè§ˆå™¨
      uc: u.indexOf('UCBrowser') > -1, //æ˜¯å¦ä¸ºandroidä¸‹çš„UCæµè§ˆå™¨
    }
  }
  var browser = {
    versions: getBrowserVersions(),
  }
  console.log('userAgent: ' + browser.versions.userAgent)

  // callback
  function fontLoaded() {
    console.log('font loaded')
    if (document.getElementsByClassName('site-intro-meta')) {
      document
        .getElementsByClassName('intro-title')[0]
        .classList.add('intro-fade-in')
      document
        .getElementsByClassName('intro-subtitle')[0]
        .classList.add('intro-fade-in')
      var postIntros = document.getElementsByClassName('post-intros')[0]
      if (postIntros) {
        postIntros.classList.add('post-fade-in')
      }
    }
  }

  // UCä¸æ”¯æŒè·¨åŸŸï¼Œæ‰€ä»¥ç›´æ¥æ˜¾ç¤º
  function asyncCb() {
    if (browser.versions.uc) {
      console.log('UCBrowser')
      fontLoaded()
    } else {
      WebFont.load({
        custom: {
          families: ['Oswald-Regular'],
        },
        loading: function () {
          // æ‰€æœ‰å­—ä½“å¼€å§‹åŠ è½½
          // console.log('font loading');
        },
        active: function () {
          // æ‰€æœ‰å­—ä½“å·²æ¸²æŸ“
          fontLoaded()
        },
        inactive: function () {
          // å­—ä½“é¢„åŠ è½½å¤±è´¥ï¼Œæ— æ•ˆå­—ä½“æˆ–æµè§ˆå™¨ä¸æ”¯æŒåŠ è½½
          console.log('inactive: timeout')
          fontLoaded()
        },
        timeout: 5000, // Set the timeout to two seconds
      })
    }
  }

  function asyncErr() {
    console.warn('script load from CDN failed, will load local script')
  }

  // load webfont-loader async, and add callback function
  function async(u, cb, err) {
    var d = document,
      t = 'script',
      o = d.createElement(t),
      s = d.getElementsByTagName(t)[0]
    o.src = u
    if (cb) {
      o.addEventListener(
        'load',
        function (e) {
          cb(null, e)
        },
        false
      )
    }
    if (err) {
      o.addEventListener(
        'error',
        function (e) {
          err(null, e)
        },
        false
      )
    }
    s.parentNode.insertBefore(o, s)
  }

  var asyncLoadWithFallBack = function (arr, success, reject) {
    var currReject = function () {
      reject()
      arr.shift()
      if (arr.length) async(arr[0], success, currReject)
    }

    async(arr[0], success, currReject)
  }

  asyncLoadWithFallBack(
    [
      'https://cdn.jsdelivr.net/npm/webfontloader@1.6.28/webfontloader.min.js',
      'https://cdn.bootcss.com/webfont/1.6.28/webfontloader.js',
      "/lib/webfontloader.min.js",
    ],
    asyncCb,
    asyncErr
  )
</script>

            <img class="loading" src="/assets/loading.svg" style="display: block; margin: 6rem auto 0 auto; width: 6rem; height: 6rem;" alt="loading">
            <div class="container container-unloaded">
                <main class="main post-page">
    <article class="article-entry">
        <!-- ## Pytorch æ¡†æ¶å­¦ä¹  -->

<p>è¯¾ç¨‹[é“¾æ¥](https: // <a target="_blank" rel="noopener" href="http://www.youtube.com/">www.youtube.com</a> / watch?v = c36lUUr864M)</p>
<h3 id="1-Tensor"><a href="#1-Tensor" class="headerlink" title="1. Tensor"></a>1. Tensor</h3><h4 id="1-1-How-to-create-tensor"><a href="#1-1-How-to-create-tensor" class="headerlink" title="1.1 How to create tensor"></a>1.1 How to create tensor</h4><blockquote>
<p>æ„é€ tensorçš„æ–¹æ³•ä¸»è¦æœ‰å…­ç§ï¼Œåˆ†åˆ«å¦‚ä¸‹ï¼š</p>
<ol>
<li><p>x = torch.ones(3, 5, dtype=torch.float, requires_grad = True )</p>
</li>
<li><p>x = torch.zeros(3, 5, dtype=torch.float, requires_grad = True )</p>
</li>
<li><p>x = torch.rand(3, 5, dtype=torch.float, requires_grad = True )</p>
</li>
<li><p>x = torch.empty(3, 5, dtype=torch.float, requires_grad = True )</p>
</li>
<li><p>x = torch.tensor([1,2,3], dtype=torch.float, requires_grad = True )</p>
</li>
<li><p>x_numpy = np.array([1,2,3])<br>x = torch.from_numpy(x_numpy)</p>
</li>
</ol>
</blockquote>
<h4 id="1-2-å¯¹Tensorçš„å„ç§æ“ä½œ"><a href="#1-2-å¯¹Tensorçš„å„ç§æ“ä½œ" class="headerlink" title="1.2 å¯¹Tensorçš„å„ç§æ“ä½œ"></a>1.2 å¯¹Tensorçš„å„ç§æ“ä½œ</h4><blockquote>
<ol>
<li><p>æŸ¥çœ‹æ•°æ®ç±»å‹ï¼šx.dtype</p>
</li>
<li><p>æŸ¥çœ‹sizeï¼šx.size()</p>
</li>
<li><p>å¦‚æœtensoråªæœ‰ä¸€ä¸ªå€¼ï¼ŒæŸ¥çœ‹å®æ•°å€¼ï¼šx.item()</p>
</li>
<li><p>æ”¹å˜tensorçš„å½¢çŠ¶å¤§å°ï¼šx = x.view(num_row, num_col)</p>
</li>
<li><p>å°†tensorè½¬ä¸ºnumpy: z = x.detach().numpy()</p>
</li>
</ol>
<p> â€‹       æ³¨æ„ï¼šnumpyåªèƒ½å­˜æ”¾åˆ°cpuä¸Šé¢,å¹¶ä¸”ä¸èƒ½æ˜¯åŒ…å«æ¢¯åº¦çš„tensorï¼Œè½¬æ¢ä¹‹åå…±äº«ç›¸åŒçš„å†…å­˜</p>
<ol start="6">
<li>æ„é€ æ–°çš„ä¸åŒ…å«æ¢¯åº¦çš„tensorï¼šz = x.detach()</li>
</ol>
<ol start="7">
<li>å¯¹tensorè¿›è¡Œå„ç§è½¬æ¢ï¼š .to() eg: x.to(int)</li>
</ol>
</blockquote>
<h4 id="1-3-åŸºæœ¬è¿ç®—"><a href="#1-3-åŸºæœ¬è¿ç®—" class="headerlink" title="1.3 åŸºæœ¬è¿ç®—"></a>1.3 åŸºæœ¬è¿ç®—</h4><blockquote>
<ol>
<li>add: z = x+y</li>
<li>minus: z = x-y</li>
<li>multi: z = x*y</li>
<li>div: z = x/y</li>
<li>åˆ‡ç‰‡æ“ä½œï¼šåˆ‡ç‰‡æ“ä½œååˆ†çµæ´»ï¼Œç”¨åˆ°çš„æ—¶å€™æŸ¥è¯¢æ–‡æ¡£</li>
</ol>
</blockquote>
<h3 id="2-Autograd"><a href="#2-Autograd" class="headerlink" title="2. Autograd"></a>2. Autograd</h3><h4 id="2-1-Calculate-the-gradients"><a href="#2-1-Calculate-the-gradients" class="headerlink" title="2.1 Calculate the gradients"></a>2.1 Calculate the gradients</h4><blockquote>
<p>ç”¨ z.backward() æ¥è®¡ç®—zå…³äºå˜é‡çš„æ¢¯åº¦ï¼Œå¦‚æœæ­£å‘è®¡ç®—å‡ºæ¥çš„zæ˜¯ä¸€ä¸ªå®æ•°å€¼ï¼Œåˆ™backwardçš„å‚æ•°ä¸ºé»˜è®¤å€¼å°±å¥½ï¼Œå¦‚æœzæ˜¯ä¸€ä¸ªå‘é‡ï¼Œåˆ™è°ƒç”¨backward()çš„æ—¶å€™éœ€è¦ä¼ å…¥ä¸€ä¸ªä¸zçš„sizeç›¸åŒçš„å‘é‡.</p>
<p>  å¦‚ä¸‹é¢çš„ç¤ºä¾‹æ‰€ç¤ºã€‚</p>
  <pre class="line-numbers language-python" data-language="python"><code class="language-python">x &#x3D; torch.randn(3, dtype&#x3D;float, requires_grad &#x3D; True)
y&#x3D;x*x+2*x+5
z &#x3D; torch.mean(y)
z.backward() # dz&#x2F;dx jacobian matrix to get the grad
# z.backward()è¿™ä¸€æ­¥æ˜¯ç”¨é›…å¯æ¯”çŸ©é˜µæ¥è®¡ç®—æ¢¯åº¦ï¼Œå¦‚æœæ­£å‘è®¡ç®—å‡ºæ¥çš„zæ˜¯
# ä¸€ä¸ªå®æ•°å€¼ï¼Œåˆ™backwardçš„å‚æ•°ä¸ºé»˜è®¤å€¼å°±å¥½ï¼Œå¦‚æœzæ˜¯ä¸€ä¸ªå‘é‡ï¼Œåˆ™è°ƒç”¨
# backward()çš„æ—¶å€™éœ€è¦ä¼ å…¥ä¸€ä¸ªä¸zçš„sizeç›¸åŒçš„å‘é‡
# 
# y &#x3D; x*x*2+x
# v &#x3D; torch.tensor([1.00,0.10,0.200], dtype&#x3D;torch.float32)
# y.backward(v)
# 
print(x.grad)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
</blockquote>
<h4 id="2-2-How-to-prevent-pytorch-from-tracking-the-history-and-calculating-this-grad-fn-attribute"><a href="#2-2-How-to-prevent-pytorch-from-tracking-the-history-and-calculating-this-grad-fn-attribute" class="headerlink" title="2.2 How to prevent pytorch from tracking the history and calculating this grad fn attribute."></a>2.2 How to prevent pytorch from tracking the history and calculating this grad fn attribute.</h4><blockquote>
<p>ä¸€å…±æœ‰ä¸‰ç§æ–¹æ³•è®©tensorä¸è¢«è®¡ç®—å›¾è¿½è¸ªæ¢¯åº¦ï¼Œåˆ†åˆ«æ˜¯ï¼š</p>
<p>1ï¼‰ x.requires_grad_(False)</p>
<p>2ï¼‰ x.detach()</p>
<p>3ï¼‰ with torch.no_grad(): pass</p>
<p>å…·ä½“ç¤ºä¾‹å¦‚ä¸‹æ‰€ç¤ºï¼š</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">x &#x3D; torch.randn(3, requires_grad &#x3D; True)
print(x)
# è®©å¼ é‡xä¸åœ¨è¿½è¸ªæ¢¯åº¦çš„æ–¹æ³•æœ‰ä¸‰ç§
# x.requires_grad_(False)
# x.detach()
# with torch.no_grad():

with torch.no_grad():
    y &#x3D; x+2
    print(x) # x with grad
    print(y) # y without grad<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>


</blockquote>
<h4 id="2-3-æ¢¯åº¦ç´¯ç§¯"><a href="#2-3-æ¢¯åº¦ç´¯ç§¯" class="headerlink" title="2.3 æ¢¯åº¦ç´¯ç§¯"></a>2.3 æ¢¯åº¦ç´¯ç§¯</h4><blockquote>
<p>è°ƒç”¨backward()æ¥è®¡ç®—æ¢¯åº¦çš„æ—¶å€™ï¼Œå½“å‰è½®æ¬¡çš„æ¢¯åº¦ï¼Œæ˜¯ä¹‹å‰æ‰€æœ‰è½®æ¬¡æ¢¯åº¦çš„ç´¯ç§¯å’Œï¼Œåœ¨æ¢¯åº¦ä¸‹é™å­¦ä¹ ä¸­è¿™æ˜¯æˆ‘ä»¬ä¸æƒ³çœ‹åˆ°çš„ï¼Œå¯ä»¥è°ƒç”¨zero_()å‡½æ•°æ¥å°†æ¢¯åº¦å˜ä¸º0.</p>
<p>ç¤ºä¾‹å¦‚ä¸‹ï¼š</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">weights &#x3D; torch.ones(3, requires_grad&#x3D;True)

for epoch in range(2):
    model_output &#x3D; (weights*3).sum()
    model_output.backward()
    print(weights.grad)
#     weights.grad.zero_()
    
# è¾“å‡ºç»“æœä¸ºï¼š
# tensor([3., 3., 3.])
# tensor([6., 6., 6.])
# ç¬¬ä¸€ä¸ªepochä¸º3ï¼Œç¬¬äºŒä¸ªepochä¸º6ï¼Œè¯´æ˜æ¢¯åº¦åœ¨ç´¯åŠ 
# å› ä¸ºæ¢¯åº¦ç´¯ç§¯æ˜¯æˆ‘ä»¬ä¸å¸Œæœ›çœ‹åˆ°çš„ï¼Œæ‰€ä»¥æ¯æ¬¡è¿­ä»£çš„æ—¶å€™æˆ‘ä»¬å¸Œæœ›æ¢¯åº¦
# èƒ½å¤Ÿæ¸…é›¶ï¼Œæ¢¯åº¦æ¸…é›¶ç”¨.grad.zero_()å‡½æ•°<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>


</blockquote>
<h3 id="3-Backpropagation"><a href="#3-Backpropagation" class="headerlink" title="3. Backpropagation"></a>3. Backpropagation</h3><blockquote>
<p>è°ƒç”¨backward()å‡½æ•°æ¥è¿›è¡Œåå‘ä¼ æ’­ï¼Œè®¡ç®—æ¢¯åº¦ï¼Œç¤ºä¾‹å¦‚ä¸‹ï¼š</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">x &#x3D; torch.tensor(1, dtype&#x3D;torch.float32, requires_grad&#x3D;False)
y &#x3D; torch.tensor(2, dtype&#x3D;torch.float32, requires_grad&#x3D;False)
w &#x3D; torch.tensor(1, dtype&#x3D;torch.float32, requires_grad&#x3D;True)

loss &#x3D; (x*w-y)**2

print(loss)

loss.backward()
print(w.grad)

## output:
# tensor(1., grad_fn&#x3D;&lt;PowBackward0&gt;)
# tensor(-2.)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>


</blockquote>
<h3 id="4-Optimize-model-with-automatic-gradient-computation"><a href="#4-Optimize-model-with-automatic-gradient-computation" class="headerlink" title="4. Optimize model with automatic gradient computation"></a>4. Optimize model with automatic gradient computation</h3><h4 id="4-1-ç”¨numpyå®ç°å›å½’ç®—æ³•"><a href="#4-1-ç”¨numpyå®ç°å›å½’ç®—æ³•" class="headerlink" title="4.1 ç”¨numpyå®ç°å›å½’ç®—æ³•"></a>4.1 ç”¨numpyå®ç°å›å½’ç®—æ³•</h4><blockquote>
<p>å®ç°å›å½’ç®—æ³•åˆ†ä¸ºå¦‚ä¸‹å‡ æ­¥ï¼š</p>
<p>â€‹    1ï¼‰ æ•°æ®å‡†å¤‡</p>
<p>â€‹    2ï¼‰å®šä¹‰function</p>
<p>â€‹    3ï¼‰å®šä¹‰loss function</p>
<p>â€‹    4ï¼‰ å®šä¹‰æ¢¯åº¦è®¡ç®—å…¬å¼</p>
<p>â€‹    5ï¼‰ ç¼–å†™training loopéƒ¨åˆ†ä»£ç </p>
<p>å®ç°ä»£ç å¦‚ä¸‹ï¼š</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># f &#x3D; w * x 

# f &#x3D; 2 * x
# 1. ç”¨numpyæ‰‹åŠ¨å®ç°

# prepare dataset
X &#x3D; np.array([1,2,3,4,5,6,7], dtype&#x3D;np.float32)
y &#x3D; np.array([2,4,6,8,9,13,20], dtype&#x3D;np.float32)

w &#x3D; 0.0

# model prediction
def forward(x):
    return w*x

# loss &#x3D; MSE
def loss(y, y_pred):
    return ((y_pred - y)**2).mean()

# gradient
def gradient(x,y,y_pred):
    return (2*x*(y_pred - y)).mean()

print(f&#39;Prediction before training: f(5) &#x3D; &#123;forward(5):.5f&#125;&#39;)

# Training
learning_rate &#x3D; 0.1
n_iters &#x3D; 10

for i in range(n_iters):
    y_pred &#x3D; forward(X)
    l &#x3D; loss(y, y_pred)
    dw &#x3D; gradient(x, y, y_pred)
    w -&#x3D; learning_rate*dw
    print(f&#39;epoch &#123;i+1&#125;: w &#x3D; &#123;w:.3f&#125;, loss &#x3D; &#123;l:.10f&#125;&#39;)
print(f&#39;Prediction after training: f(5) &#x3D; &#123;forward(5):.5f&#125;&#39;)
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>


</blockquote>
<h4 id="4-2-ç”¨pytorchæ›¿æ¢æ•°æ®ç±»å‹å’Œæ¢¯åº¦è®¡ç®—"><a href="#4-2-ç”¨pytorchæ›¿æ¢æ•°æ®ç±»å‹å’Œæ¢¯åº¦è®¡ç®—" class="headerlink" title="4.2 ç”¨pytorchæ›¿æ¢æ•°æ®ç±»å‹å’Œæ¢¯åº¦è®¡ç®—"></a>4.2 ç”¨pytorchæ›¿æ¢æ•°æ®ç±»å‹å’Œæ¢¯åº¦è®¡ç®—</h4><blockquote>
<p>ä»£ç å®ç°æ€è·¯ä¸4.1ä¸€æ ·ï¼Œæ²¡æœ‰åšä»»ä½•å˜åŒ–ã€‚</p>
<p>ç¤ºä¾‹ä»£ç å¦‚ä¸‹ï¼š</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 2. Do with pytorch
X &#x3D; torch.tensor([1,2,3,4,5,6,10], dtype&#x3D;torch.float32)
y &#x3D; torch.tensor([2,4,6,8,10,12,20], dtype&#x3D;torch.float32)

w &#x3D; torch.tensor(0.0, dtype&#x3D;torch.float32, requires_grad &#x3D; True)

# model prediction
def forward(x):
    return w*x

# loss &#x3D; MSE
def loss(y, y_pred):
    return ((y_pred - y)**2).mean()

print(f&#39;Prediction before training: f(5) &#x3D; &#123;forward(5):.5f&#125;&#39;)

# Training
learning_rate &#x3D; 0.0001
n_iters &#x3D; 10000

for i in range(n_iters):
    y_pred &#x3D; forward(X)
    
    l &#x3D; loss(y, y_pred)
    
    l.backward()
    
    with torch.no_grad():
        w -&#x3D; learning_rate*w.grad
    
    w.grad.zero_()
    
    if (i+1) %1000 &#x3D;&#x3D; 0:
        print(f&#39;epoch &#123;i+1&#125;: w &#x3D; &#123;w:.3f&#125;, loss &#x3D; &#123;l:.10f&#125;&#39;)
print(f&#39;Prediction after training: f(5) &#x3D; &#123;forward(5):.5f&#125;&#39;)
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>


</blockquote>
<h4 id="4-3-ç”¨pytorchæ¥åšæ¢¯åº¦è®¡ç®—å’Œä¼˜åŒ–"><a href="#4-3-ç”¨pytorchæ¥åšæ¢¯åº¦è®¡ç®—å’Œä¼˜åŒ–" class="headerlink" title="4.3 ç”¨pytorchæ¥åšæ¢¯åº¦è®¡ç®—å’Œä¼˜åŒ–"></a>4.3 ç”¨pytorchæ¥åšæ¢¯åº¦è®¡ç®—å’Œä¼˜åŒ–</h4><blockquote>
<p>4.2 æˆ‘ä»¬ç”¨äº†backward()å‡½æ•°æ¥è‡ªåŠ¨è®¡ç®—æ¢¯åº¦ï¼Œä½†æ˜¯æ¢¯åº¦ä¸‹é™ä¼˜åŒ–ç®—æ³•ä¾ç„¶æ˜¯æˆ‘ä»¬åœ¨4.1ç‰ˆæœ¬ä¸­çš„æ‰‹å·¥ç¼–å†™çš„å…¬å¼ï¼Œè¿™é‡Œæˆ‘ä»¬åœ¨ä¹‹å‰çš„åŸºç¡€ä¸Šé¢æ›´è¿›ä¸€æ­¥ï¼Œç”¨pytorchæ¡†æ¶ä¸­çš„optimizeræ¥æ›¿æ¢æ‰æ‰‹å·¥ç¼–å†™çš„æ¢¯åº¦ä¸‹é™æ–¹æ³•ï¼Œä»£ç å¦‚ä¸‹ï¼š</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 1ï¼‰ Design model (input, output size, forward pass)
# 2) Construct loss and optimizer
# 3) Training loop
#    - forward pass: compute prediction
#    - backward pass: gradients
#    - update weight

import torch.nn as nn

X &#x3D; torch.tensor([[1],[2],[3],[4],[5],[6],[10]], dtype&#x3D;torch.float32)
y &#x3D; torch.tensor([[2],[4],[6],[8],[10],[12],[20]], dtype&#x3D;torch.float32)
x_test &#x3D; torch.tensor([5], dtype&#x3D;torch.float32)

n_samples, n_features &#x3D; X.shape
print(n_samples, n_features)

input_size &#x3D; n_features
output_size &#x3D; n_features

model &#x3D; nn.Linear(input_size, output_size)


print(f&#39;Prediction before training: f(5) &#x3D; &#123;model(x_test).item():.5f&#125;&#39;)

# Training
learning_rate &#x3D; 0.01
n_iters &#x3D; 1000

loss &#x3D; nn.MSELoss()
optimizer &#x3D; torch.optim.SGD(model.parameters(), lr&#x3D;learning_rate)

for i in range(n_iters):
    y_pred &#x3D; model(X)
    
    l &#x3D; loss(y, y_pred)
    
    l.backward()
    optimizer.step()
    
    
#     w.grad.zero_()
    optimizer.zero_grad()
    
    if (i+1) %1000 &#x3D;&#x3D; 0:
        [w, b] &#x3D; model.parameters()
        print(f&#39;epoch &#123;i+1&#125;: w &#x3D; &#123;w[0][0].item():.3f&#125;, loss &#x3D; &#123;l:.10f&#125;&#39;)
print(f&#39;Prediction after training: f(5) &#x3D; &#123;model(x_test).item():.5f&#125;&#39;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
</blockquote>
<h4 id="4-4-å®šä¹‰æ›´åŠ å¤æ‚çš„æ¨¡å‹"><a href="#4-4-å®šä¹‰æ›´åŠ å¤æ‚çš„æ¨¡å‹" class="headerlink" title="4.4 å®šä¹‰æ›´åŠ å¤æ‚çš„æ¨¡å‹"></a>4.4 å®šä¹‰æ›´åŠ å¤æ‚çš„æ¨¡å‹</h4><blockquote>
<p>4.3 åŠå…¶ä¹‹å‰çš„å·¥ä½œï¼Œæˆ‘ä»¬å¯¹modelçš„å®šä¹‰éƒ½åŠå…¶çš„ç®€å•ï¼Œè¿™é‡Œæˆ‘ä»¬å¯¹modelè¿›è¡Œæ”¹è¿›ï¼Œé€šè¿‡è‡ªå®šä¹‰ä¸€ä¸ªLinearRegresså¯¹è±¡æ¥å®šä¹‰ä¸€ä¸ªåŒ…å«ä¸¤å±‚å…¨è¿æ¥çš„ç½‘ç»œå»ºæ¨¡æˆ‘ä»¬çš„modelï¼Œéšè—å±‚çš„æ¿€æ´»å‡½æ•°æˆ‘ä»¬é€‰ç”¨äº†relu()æ¿€æ´»å‡½æ•°ã€‚ä»£ç å¦‚ä¸‹ï¼š</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 1ï¼‰ Design model (input, output size, forward pass)
# 2) Construct loss and optimizer
# 3) Training loop
#    - forward pass: compute prediction
#    - backward pass: gradients
#    - update weight

import torch.nn as nn

X &#x3D; torch.tensor([[1],[2],[3],[4],[5],[6],[10]], dtype&#x3D;torch.float32)
y &#x3D; torch.tensor([[2],[4],[6],[8],[10],[12],[20]], dtype&#x3D;torch.float32)
x_test &#x3D; torch.tensor([5], dtype&#x3D;torch.float32)

n_samples, n_features &#x3D; X.shape
print(n_samples, n_features)

input_size &#x3D; n_features
output_size &#x3D; n_features

# model &#x3D; nn.Linear(input_size, output_size)

class LinearRegress(nn.Module):
    def __init__(self, input_size, output_size):
        super(LinearRegress, self).__init__()
        # define layers
        self.lin1 &#x3D; nn.Linear(input_size, 2)
        self.lin2 &#x3D; nn.Linear(2, output_size)
    def forward(self, x):
        x &#x3D;  self.lin1(x)
        x &#x3D; torch.relu(x)
        return self.lin2(x)
        

model &#x3D; LinearRegress(input_size, output_size)


print(f&#39;Prediction before training: f(5) &#x3D; &#123;model(x_test).item():.5f&#125;&#39;)

# Training
learning_rate &#x3D; 0.01
n_iters &#x3D; 1000

loss &#x3D; nn.MSELoss()
optimizer &#x3D; torch.optim.SGD(model.parameters(), lr&#x3D;learning_rate)

for i in range(n_iters):
    y_pred &#x3D; model(X)
    
    l &#x3D; loss(y, y_pred)
    
    l.backward()
    optimizer.step()
    
    
#     w.grad.zero_()
    optimizer.zero_grad()
    
    if (i+1) %1000 &#x3D;&#x3D; 0:
#         [w, b] &#x3D; model.parameters()
#         print(f&#39;epoch &#123;i+1&#125;: parameters &#x3D; &#123;model.parameters()&#125;, loss &#x3D; &#123;l:.10f&#125;&#39;)
        for parameters in model.parameters():
            print(parameters)
        
print(f&#39;Prediction after training: f(5) &#x3D; &#123;model(x_test).item():.5f&#125;&#39;)

# Now pytorch can do most of the work for us, of course we still have to design our model
# and have to know which loss and optimizer we want to use but we don&#39;t have to worry about
# the underlying algorithms anymore.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>


</blockquote>
<h4 id="4-5-ç»¼åˆç¤ºä¾‹1"><a href="#4-5-ç»¼åˆç¤ºä¾‹1" class="headerlink" title="4.5 ç»¼åˆç¤ºä¾‹1"></a>4.5 ç»¼åˆç¤ºä¾‹1</h4><blockquote>
<p>å­¦ä¹ å®Œäº†4.1-4.3çš„å†…å®¹ï¼Œæˆ‘ä»¬å¯¹å¦‚ä½•æ‰‹å·¥å»ºæ¨¡ä¸€ä¸ªæ¨¡å‹æœ‰äº†åˆæ­¥çš„äº†è§£ï¼Œå¹¶ä¸”çŸ¥é“äº†pytorchå»ºæ¨¡æ·±åº¦å­¦ä¹ çš„ä¸€èˆ¬è¿‡ç¨‹ï¼Œä»¥åŠå®ƒå¯¹åº”çš„æ¯ä¸€ä¸ªéƒ¨åˆ†çš„ä½œç”¨ï¼Œä¸‹é¢æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªç»¼åˆçš„ç¤ºä¾‹æ¥åŠ æ·±å¯¹æ‰€å­¦çŸ¥è¯†çš„ç†è§£ã€‚</p>
<p>åœ¨è¯¥ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆè°ƒç”¨sklearnçš„datasetsåº“æ¥æ„å»ºå¸¦å™ªå£°çš„é€»è¾‘å›å½’æ•°æ®é›†ï¼Œç„¶åæ„é€ ä¸€ä¸ªçº¿æ€§æ¨¡å‹ï¼Œå¹¶å¯¹çº¿æ€§æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚ä»£ç å¦‚ä¸‹ï¼š</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 5. ç»¼åˆç¤ºä¾‹1

import torch
import torch.nn as nn
import numpy as np
from sklearn import datasets
import matplotlib.pyplot as plt

# 0) create data
x_numpy, y_numpy &#x3D; datasets.make_regression(n_samples&#x3D;500, n_features&#x3D;1, noise&#x3D;20, random_state&#x3D;1)

x,y &#x3D; torch.from_numpy(x_numpy.astype(np.float32)), torch.from_numpy(y_numpy.astype(np.float32))
# print(x.shape)
# print(y.shape)
y &#x3D; y.view(y.shape[0], -1)
# print(y.shape)
n_sample, n_features &#x3D; x.shape

# 1) model
input_size &#x3D; n_features
output_size &#x3D; 1
model &#x3D; nn.Linear(input_size, output_size)

# 2ï¼‰loss and optimizer
learning_rate &#x3D; 0.01
criterion &#x3D; nn.MSELoss()
optimizer &#x3D; torch.optim.SGD(model.parameters(), lr&#x3D;learning_rate)

# 3) training loop
num_epochs &#x3D; 1000
num_iter &#x3D; 100
for epoch in range(num_epochs):
 # foreard pass and loss
 y_pred &#x3D; model(x)
 loss &#x3D; criterion(y_pred, y)
 # backward pass
 loss.backward()
 # updata
 optimizer.step()

 optimizer.zero_grad()

 if(epoch+1)%num_iter &#x3D;&#x3D; 0:
     print(f&#39;epoch:&#123;epoch+1&#125;, loss&#x3D;&#123;loss.item():.4f&#125;&#39;)
# plot
predicted &#x3D; model(x).detach().numpy()
plt.plot(x_numpy, y_numpy, &#39;ro&#39;)
plt.plot(x_numpy, predicted, &#39;b&#39;)
plt.show()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>è¿è¡Œç»“æœå¦‚ä¸‹ï¼š</p>
<img src="https://raw.githubusercontent.com/Skylyong/i/main/20210805210347.png" alt="image-20210805210341931" style="zoom:50%;" />
</blockquote>
<h4 id="4-6-ç»¼åˆç¤ºä¾‹2"><a href="#4-6-ç»¼åˆç¤ºä¾‹2" class="headerlink" title="4.6  ç»¼åˆç¤ºä¾‹2"></a>4.6  ç»¼åˆç¤ºä¾‹2</h4><blockquote>
<p>ä¹‹å‰çš„ç¤ºä¾‹ä¸­æˆ‘ä»¬æ²¡æœ‰å¯¹è®­ç»ƒåçš„æ¨¡å‹è¿›è¡Œæµ‹è¯•ï¼Œåœ¨è¿™ä¸ªä¾‹å­ä¸­æˆ‘ä»¬å¢åŠ äº†æ¨¡å‹æµ‹è¯•ï¼Œå¹¶ä¸”è°ƒç”¨sklearnä¸­å†…ç½®çš„â€œä¹³è…ºç™Œâ€æ•°æ®é›†æ¥å®Œæˆç¤ºä¾‹ï¼Œå¹¶ä¸”å¯¹è¾“å…¥ç‰¹å¾åšäº†å½’ä¸€åŒ–å¤„ç†ã€‚</p>
<p>ä»£ç å¦‚ä¸‹ï¼š</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 5. ç»¼åˆç¤ºä¾‹2

import torch
import torch.nn as nn
import numpy as np
from sklearn import datasets
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# 0) prepare data
bc &#x3D; datasets.load_breast_cancer()
X,y &#x3D; bc.data, bc.target

n_samples, n_features &#x3D; X.shape
# print(X.shape)
# print(y.shape)

X_train, X_test, y_train, y_test &#x3D; train_test_split(X,y,test_size&#x3D;0.2, random_state&#x3D;1234)
# scale å¯¹æ¯ä¸€åˆ—çš„ç‰¹å¾åšå‡å€¼æ–¹å·®å½’ä¸€åŒ–å¤„ç†ï¼Œå¦‚æœä¸åšçš„è¯å­¦ä¹ å‡ºæ¥çš„æ¨¡å‹çš„å‡†ç¡®ç‡å¤§å¤§é™ä½
sc &#x3D; StandardScaler()
# print(&#39;X_test before transform:&#39;, X_test)
X_train &#x3D; sc.fit_transform(X_train)
X_test &#x3D; sc.fit_transform(X_test)
# print(&#39;X_test after transform:&#39;, X_test.mean(), X_test.std())

X_train &#x3D; torch.from_numpy(X_train.astype(np.float32))
X_test &#x3D; torch.from_numpy(X_test.astype(np.float32))
y_train &#x3D; torch.from_numpy(y_train.astype(np.float32))
y_test &#x3D; torch.from_numpy(y_test.astype(np.float32))

y_train &#x3D; y_train.view(y_train.shape[0], 1)
y_test &#x3D; y_test.view(y_test.shape[0], 1)

# 1) model
# f &#x3D; wx + b, sigmoid at the end
class LogisticRegression(nn.Module):
    def __init__(self, n_input_features):
        super(LogisticRegression, self).__init__()
        self.linear &#x3D; nn.Linear(n_input_features, 1)
        
    def forward(self, x):
        y_pred &#x3D; torch.sigmoid(self.linear(x))
        return y_pred
    
model &#x3D; LogisticRegression(n_features)

# 2) loss and optimizer
learning_rate &#x3D; 0.001

criterion &#x3D; nn.BCELoss()
optimizer &#x3D; torch.optim.SGD(model.parameters(), lr &#x3D; learning_rate)
# 3) training loop
num_epochs &#x3D; 1000
for epoch in range(num_epochs):
    # foreard pass and loss
    y_pred &#x3D; model(X_train)
    loss &#x3D; criterion(y_pred, y_train)
    # backward pass
    loss.backward()
    
    # updates
    optimizer.step()
    
    optimizer.zero_grad()
    
    if(epoch+1) % 100 &#x3D;&#x3D; 0:
        print(f&#39;epoch:&#123;epoch+1&#125;, loss &#x3D; &#123;loss.item():.4f&#125;&#39;)
        
with torch.no_grad():
    y_pred &#x3D; model(X_test)
    y_pred_cls &#x3D; y_pred.round() # å¯¹sigmoidå‡ºæ¥çš„å€¼è¿›è¡Œå››èˆäº”å…¥
#     print(f&#39;y_pred&#x3D;&#123;y_pred&#125;, y_pred_cls&#x3D;&#123;y_pred_cls&#125;&#39;)
    acc &#x3D; y_pred_cls.eq(y_test).sum()&#x2F;float(y_test.shape[0])
    print(f&#39;acc &#x3D; &#123;acc:.4f&#125;&#39;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>


</blockquote>
<h3 id="5-Dataset-and-Dataload-Class"><a href="#5-Dataset-and-Dataload-Class" class="headerlink" title="5. Dataset and Dataload Class"></a>5. Dataset and Dataload Class</h3><h4 id="5-1-Dataset-and-Dataload"><a href="#5-1-Dataset-and-Dataload" class="headerlink" title="5.1 Dataset and Dataload"></a>5.1 Dataset and Dataload</h4><blockquote>
<p>pytorchä¸­ç”±datasetç±»ç®¡ç†æ•°æ®é›†ï¼Œå¯ä»¥é€šè¿‡ç»§æ‰¿datasetç±»æ¥è‡ªå®šä¹‰æ•°æ®é›†ï¼Œä»£ç æ¨¡æ¿å¦‚ä¸‹ï¼š</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">class MyDataset(Dataset):
def __init__(self):
 pass

def __getitem__(self, index):
 return a signel data

def __len__(self):
 return length of dataset<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>å®šä¹‰å¥½datasetä¹‹åï¼Œå¯ä»¥é€šè¿‡DataLoaderæ¥æ„é€ å¯ä¾›è®­ç»ƒçš„è¿­ä»£å™¨ï¼Œä»£ç å¦‚ä¸‹ï¼š</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">dataloader &#x3D; DataLoader(dataset&#x3D;dataset, batch_size&#x3D;4, shuffle&#x3D;True, num_workers&#x3D;2)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>



<p>ç¤ºä¾‹å¦‚ä¸‹æ‰€ç¤ºï¼š</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">&#39;&#39;&#39;
epoch &#x3D; 1 forward and backward pass of ALL training samples

bach_size &#x3D; number of training samples in one forward &amp; backward pass

number of iterations &#x3D; number of passes, each pass using [batch_size] number of samples

e.g. 100 samples, batch_size &#x3D; 20 --&gt; 100&#x2F;20 &#x3D; 5 iterations for 1 epoch
&#39;&#39;&#39;

import torch
import torchvision
from torch.utils.data import Dataset, DataLoader
import numpy as np
import math
import os

if not os.getcwd().endswith(&#39;pytorchLearning&#39;):
 os.chdir(os.getcwd()+&#39;&#x2F;Desktop&#x2F;pytorchLearning&#39;)
# print (os.getcwd())

class WineDataset(Dataset):

 def __init__(self):
     # data loading
     xy &#x3D; np.loadtxt(&#39;.&#x2F;data&#x2F;wine&#x2F;wine.csv&#39;, delimiter&#x3D;&#39;,&#39;, dtype&#x3D;np.float32, skiprows&#x3D;1)
     self.x &#x3D; torch.from_numpy(xy[:,1:])
     self.y &#x3D; torch.from_numpy(xy[:, [0]])
     self.n_samples &#x3D; xy.shape[0]

 def __getitem__(self, index):
     return self.x[index], self.y[index]

 def __len__(self):
     return self.n_samples
# # How we can use dataset
# dataset &#x3D; WineDataset()
# first_data &#x3D; dataset[0]
# features, label &#x3D; first_data
# print(features, label)
#
# # How we can use dataload
# dataloader &#x3D; DataLoader(dataset&#x3D;dataset, batch_size&#x3D;4, shuffle&#x3D;True, num_workers&#x3D;2)
#
# dataiter &#x3D; iter(dataloader)
# data &#x3D; dataiter.next()
# features, label &#x3D; data
# print(features, label)

dataset &#x3D; WineDataset()
dataloader &#x3D; DataLoader(dataset&#x3D;dataset, batch_size&#x3D;4, shuffle&#x3D;True, num_workers&#x3D;2)

# training loop
num_epochs&#x3D;2
total_samples &#x3D; len(dataset)
n_iterations &#x3D; math.ceil(total_samples&#x2F;4)
print(total_samples, n_iterations)

for epoch in range(num_epochs):
 for i,(inputs, label) in enumerate(dataloader):
     # forward backward, update
     if (i+1) % 5 &#x3D;&#x3D; 0:
         print(f&#39;epoch &#123;epoch+1&#125;&#x2F;&#123;num_epochs&#125;, step &#123;i+1&#125;&#x2F;&#123;n_iterations&#125;, inputs &#123;inputs.shape&#125;&#39;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>


</blockquote>
<h4 id="5-2-Transforms-for-the-dataset"><a href="#5-2-Transforms-for-the-dataset" class="headerlink" title="5.2 Transforms for the dataset"></a>5.2 Transforms for the dataset</h4><blockquote>
<p>pytorché€šè¿‡ä¼ å…¥transformsç±»æ¥å¯¹è¾“å…¥çš„æ•°æ®è¿›è¡ŒæŸç§å˜åŒ–ï¼Œtransformsç±»å¯ä»¥è°ƒç”¨åº“å®šä¹‰å¥½çš„ï¼Œä¹Ÿå¯ä»¥æ ¹æ®éœ€è¦è‡ªå®šä¹‰ã€‚</p>
<p>ç¤ºä¾‹ä»£ç å¦‚ä¸‹ï¼š</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">&#39;&#39;&#39;
Transforms can be applied to PIL images, tensor, ndarrays, or custom data
during criterion of the dataset

complete list of built-in transforms:
https:&#x2F;&#x2F;pytorch.org&#x2F;docs&#x2F;stable&#x2F;torchvision&#x2F;transforms.html

On images
---------
CenterCrop, Grayscale, Pad, RandomAffine,
RandomCrop, RandomHorizontalFlip, RandomHorizon
Resize, scale

On Tensors
----------
LinearTransformation, Normalize, RandomErasing

Conversion
----------
ToPILIamage: from tensor or ndarray
ToTensor: from numpy.ndarray or PILImage

Generic
-------
Use Lambda

Custom
------
Write own Class

Compose mutiple Transforms
--------------------------
composed &#x3D; transforms.Compose([Rescale(256),
                                RandomCrop(224)])
torchvision.transforms.Rescale(256)
torchvision.transforms.ToTensor()
&#39;&#39;&#39;
import torch
import torchvision
from torch.utils.data import Dataset
import numpy as np


class WineDataset(Dataset):

    def __init__(self, transform&#x3D;None):
        xy &#x3D; np.loadtxt(&#39;.&#x2F;data&#x2F;wine&#x2F;wine.csv&#39;, delimiter&#x3D;&#39;,&#39;, dtype&#x3D;np.float32, skiprows&#x3D;1)
        self.n_samples &#x3D; xy.shape[0]

        # note that we do not convert to tensor here
        self.x &#x3D; xy[:,1:]
        self.y &#x3D; xy[:, [0]]

        self.transform &#x3D; transform

    def __getitem__(self, index):
        sample &#x3D; self.x[index], self.y[index]

        if self.transform:
            sample &#x3D; self.transform(sample)

        return sample

    def __len__(self):
        return self.n_samples

class ToTensor:
    def __call__(self, sample):
        inputs, targets &#x3D; sample
        return torch.from_numpy(inputs), torch.from_numpy(targets)

class MulTransform:
    def __init__(self, factor):
        self.factor &#x3D; factor

    def __call__(self, sample):
        inputs, target &#x3D; sample
        inputs *&#x3D; self.factor
        return inputs, target

dataset &#x3D; WineDataset(transform&#x3D;ToTensor())
first_data &#x3D; dataset[0]
features, label &#x3D; first_data
print(features)
print(type(features), type(label))


composed &#x3D; torchvision.transforms.Compose([ToTensor(), MulTransform(2)]) # æŠŠä¸¤ä¸ªtransformsåˆå¹¶ä¸€èµ·åº”ç”¨
dataset &#x3D; WineDataset(transform&#x3D;composed)
first_data &#x3D; dataset[0]
features, label &#x3D; first_data
print(features)
print(type(features), type(label))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
</blockquote>
<h3 id="6-Softmax-and-Cross-Entropy"><a href="#6-Softmax-and-Cross-Entropy" class="headerlink" title="6. Softmax and Cross-Entropy"></a>6. Softmax and Cross-Entropy</h3><h4 id="6-1-Softmax"><a href="#6-1-Softmax" class="headerlink" title="6.1 Softmax"></a>6.1 Softmax</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"># Softmax
import torch
import torch.nn as nn
import numpy as np

def softmax(x):
    return np.exp(x) &#x2F; np.sum(np.exp(x), axis&#x3D;0)

x &#x3D; np.array([-2, 1, 0.1])
outputs &#x3D; softmax(x)
print(&#39;softmax numpy:&#39;, outputs)

x &#x3D; torch.from_numpy(x)
outputs &#x3D; torch.softmax(x, axis&#x3D;0)
print(&#39;softmax torch:&#39;, outputs)
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>



<h4 id="6-2-Cross-entropy"><a href="#6-2-Cross-entropy" class="headerlink" title="6.2 Cross entropy"></a>6.2 Cross entropy</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"># Cross entropy

def cross_entropy(actual, predicted):
    return -np.sum(actual*np.log(predicted)).mean()

# y must be one hot encoded
# if class 0: [1 0 0]
# if class 1: [0 1 0]
# if class 2: [0 0 1]
Y &#x3D; np.array([1,0,0])

# y_pred has probabilities
y_pred_good &#x3D; np.array([0.7,0.2,0.1])
y_pred_bad &#x3D; np.array([0.1, 0.3, 0.6])
l1 &#x3D; cross_entropy(Y, y_pred_good)
l2 &#x3D; cross_entropy(Y, y_pred_bad)
print(f&#39;Loss1 numpy:&#123;l1:.4f&#125;&#39;)
print(f&#39;Loss2 numpy:&#123;l2:.4f&#125;&#39;)


# cross entropy by pytorch
# in pytorch: Use nn.CrossEntropyLoss()
# No softmax at the end!
loss &#x3D; nn.CrossEntropyLoss()

# nsamples x nclasses &#x3D; 1x3
y &#x3D; torch.tensor([0])
y_pred_good &#x3D; torch.tensor([[2.0, 1.0, 0.1]])
y_pred_bad &#x3D; torch.tensor([[0.5, 2.0, 0.3]])

l1 &#x3D; loss(y_pred_good, y)
l2 &#x3D; loss(y_pred_bad, y)
print(f&#39;Loss1 numpy:&#123;l1:.4f&#125;&#39;)
print(f&#39;Loss2 numpy:&#123;l2:.4f&#125;&#39;)

_, prediction1 &#x3D; torch.max(y_pred_good, 1)
_, prediction2 &#x3D; torch.max(y_pred_bad, 1)
print(prediction1)
print(prediction2)


# 3 samples
y &#x3D; torch.tensor([2,0,1])
y_pred_good &#x3D; torch.tensor([[0.1, 1.0, 2.0],
                            [2.0, 1.0, 0.5 ],
                            [0.1, 2.0, 1.0]])
y_pred_bad &#x3D; torch.tensor([[2.0, 1.0, 0.1],
                            [1.0, 0.1, 2.0 ],
                            [0.1, 1.0, 2.0]])

l1 &#x3D; loss(y_pred_good, y)
l2 &#x3D; loss(y_pred_bad, y)
print(f&#39;Loss1 numpy:&#123;l1.item():.4f&#125;&#39;)
print(f&#39;Loss2 numpy:&#123;l2.item():.4f&#125;&#39;)

_, prediction1 &#x3D; torch.max(y_pred_good, 1)
_, prediction2 &#x3D; torch.max(y_pred_bad, 1)
print(prediction1)
print(prediction2)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>



<h3 id="7-Activation-function"><a href="#7-Activation-function" class="headerlink" title="7. Activation function"></a>7. Activation function</h3><blockquote>
<p>Activation functions apply a non-linear transform and decide whether a neural should be activated or not.</p>
<h4 id="Most-popular-activation-functions"><a href="#Most-popular-activation-functions" class="headerlink" title="Most popular activation functions"></a>Most popular activation functions</h4><ol>
<li>Step function â€“&gt; Not used in practice</li>
<li>Sigmoid</li>
<li>TanH</li>
<li>ReLU</li>
<li>Leaky ReLU</li>
<li>Softmax</li>
</ol>
</blockquote>
<h3 id="8-ç»¼åˆç»ƒä¹ -MNIST"><a href="#8-ç»¼åˆç»ƒä¹ -MNIST" class="headerlink" title="8. ç»¼åˆç»ƒä¹  MNIST"></a>8. ç»¼åˆç»ƒä¹  MNIST</h3><blockquote>
<ol>
<li><p>MNIST</p>
</li>
<li><p>DataLoader, Transformation</p>
</li>
<li><p>Multilayer Neural Net, activation function</p>
</li>
<li><p>Loss and Optimizer</p>
</li>
<li><p>Tra</p>
<p>ä»£ç å¦‚ä¸‹ï¼š</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import torch.nn.functional as F
import math
                                 
# device config
from torch.optim import optimizer
                                 
device &#x3D;torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)
                                 
# hyper parameters
input_size &#x3D; 784 # 28x28
hidden_size &#x3D; 128
num_classes &#x3D; 10
num_epochs &#x3D; 2
batch_size &#x3D; 100
learning_rate &#x3D; 0.001
                                 
# MNIST DataSet
train_dataset &#x3D; torchvision.datasets.MNIST(root&#x3D;&#39;.&#x2F;data&#39;, train&#x3D;True,
                                           transform&#x3D;transforms.ToTensor(),download&#x3D;True)
test_dataset &#x3D; torchvision.datasets.MNIST(root&#x3D;&#39;.&#x2F;data&#39;, train&#x3D;False,
                                           transform&#x3D;transforms.ToTensor())
train_loader &#x3D; torch.utils.data.DataLoader(dataset&#x3D;train_dataset,batch_size&#x3D;batch_size,
                                           shuffle&#x3D;True)
test_loader &#x3D; torch.utils.data.DataLoader(dataset&#x3D;test_dataset,batch_size&#x3D;batch_size,
                                           shuffle&#x3D;False)
examples &#x3D; iter(train_loader)
features, label &#x3D; examples.next()
print(features.shape)
print(label.shape)
                                 
for i in range(6):
    plt.subplot(2,3, i+1)
    plt.imshow(features[i][0], cmap&#x3D;&#39;gray&#39;)
plt.show()
                                 
# model
class NeuralNet(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super(NeuralNet, self).__init__()
        self.l1 &#x3D; nn.Linear(input_size, hidden_size)
        self.l2 &#x3D; nn.Linear(hidden_size, num_classes)
                                 
    def forward(self, x):
        out &#x3D; F.relu(self.l1(x))
        out &#x3D; self.l2(out)
        return out
                                 
model &#x3D; NeuralNet(input_size, hidden_size, num_classes)
# loss and optimizer
criterion &#x3D; nn.CrossEntropyLoss()
optimizer &#x3D; torch.optim.Adam(model.parameters(), lr&#x3D;learning_rate)
                                 
# training loop
n_total_steps &#x3D; len(train_loader)
for epoch in range(num_epochs):
    for i, (images, label) in enumerate(train_loader):
        # 100, 1, 28, 28
        # 100, 784
        images &#x3D; images.reshape(-1, 28*28).to(device)
        label &#x3D; label.to(device)
                                 
        # forward
        outputs &#x3D; model(images)
        loss &#x3D; criterion(outputs, label)
                                 
        # backward
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
                                 
        if (i+1) % 100 &#x3D;&#x3D;0:
            print(f&#39;epoch &#123;epoch+1&#125; &#x2F; &#123;num_epochs&#125;, step &#123;i+1&#125; &#x2F; &#123;n_total_steps&#125;, loss &#123;loss.item():.4f&#125;&#39;)
                                 
# test
with torch.no_grad():
    n_correct &#x3D; 0
    n_sample &#x3D; 0
    for images, label in test_loader:
        images &#x3D; images.reshape(-1, 28*28).to(device)
        label &#x3D; label.to(device)
        outputs &#x3D; model(images)
        # print(outputs)
                                 
        # value, index
        _, prdiction &#x3D; torch.max(outputs, 1)
        n_correct +&#x3D; torch.sum(torch.eq(prdiction, label)).item()
        n_sample +&#x3D; label.shape[0]
                                 
    print(f&#39;Acc: &#123;100.0*n_correct &#x2F; n_sample&#125;&#39;)
                                 <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>è¿è¡Œç»“æœï¼š</p>
<img src="https://raw.githubusercontent.com/Skylyong/i/main/20210805213343.png" alt="image-20210805213343033" style="zoom:50%;" /></li>
</ol>
</blockquote>
<h3 id="9-Convolutional-Neural-Net-CNN"><a href="#9-Convolutional-Neural-Net-CNN" class="headerlink" title="9.  Convolutional Neural Net(CNN)"></a>9.  Convolutional Neural Net(CNN)</h3><blockquote>
<p>è¿™é‡Œæˆ‘ä»¬é€šè¿‡æ„é€ CNNæ¨¡å‹å¹¶å¯¹å›¾ç‰‡è¿›è¡Œåˆ†ç±»ï¼Œé‡‡ç”¨â€œCIFAR10â€æ•°æ®é›†ï¼Œç„¶åè‡ªå®šä¹‰Modelï¼Œè°ƒç”¨â€œäº¤å‰ç†µæŸå¤±å‡½æ•°â€å’Œâ€œSGDâ€ä¼˜åŒ–å™¨æ¥å¯¹æ¨¡å‹è®­ç»ƒï¼Œæ¨¡å‹è®­ç»ƒç»“æŸä¹‹åå¯¹åˆ†ç±»å™¨çš„åˆ†ç±»å‡†ç¡®ç‡è¿›è¡Œäº†æµ‹è¯•ã€‚</p>
<p>å®éªŒä»£ç å¦‚ä¸‹ï¼š</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import numpy as np
import torch.nn.functional as F

# device config
device &#x3D;torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)

# hyper parameters
num_epochs &#x3D; 4
batch_size &#x3D; 4
learning_rate &#x3D;0.001

# dataset has PILImage images of range[0,1]
# We transform them to Tensor of normalised range[-1, 1]
transform &#x3D; transforms.Compose(
    [
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))
    ]
)

train_dataset &#x3D; torchvision.datasets.CIFAR10(root&#x3D;&#39;.&#x2F;data&#39;, train&#x3D;True,
                                             download&#x3D;True, transform&#x3D;transform)
test_dataset &#x3D; torchvision.datasets.CIFAR10(root&#x3D;&#39;.&#x2F;data&#39;, train&#x3D;False,
                                             download&#x3D;True, transform&#x3D;transform)
train_loader &#x3D; torch.utils.data.DataLoader(dataset&#x3D;train_dataset, batch_size&#x3D;batch_size,
                                           shuffle&#x3D;True)
test_loader &#x3D; torch.utils.data.DataLoader(dataset&#x3D;test_dataset, batch_size&#x3D;batch_size,
                                           shuffle&#x3D;False)

classes &#x3D;(&#39;plane&#39;, &#39;car&#39;,&#39;bird&#39;,&#39;cat&#39;,&#39;deer&#39;,&#39;dog&#39;,&#39;frog&#39;,
          &#39;horse&#39;,&#39;ship&#39;,&#39;truck&#39;)

def imshow(img):
    img &#x3D; img &#x2F; 2 + 0.5  # unnormalize
    npimg &#x3D; img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()

# model
class ConvNet(nn.Module):
    def __init__(self):
        super(ConvNet, self).__init__()
        self.conv1 &#x3D; nn.Conv2d(3,6,5)
        self.pool &#x3D; nn.MaxPool2d(2,2)
        self.conv2 &#x3D; nn.Conv2d(6,16,5)
        self.fc1 &#x3D; nn.Linear(16*5*5, 120)
        self.fc2 &#x3D; nn.Linear(120, 84)
        self.fc3 &#x3D; nn.Linear(84, 10)

    def forward(self,x):
        x &#x3D; self.pool( F.relu(self.conv1(x)))
        x &#x3D; self.pool(F.relu(self.conv2(x)))
        x &#x3D; x.view(-1,16*5*5)
        x &#x3D; F.relu(self.fc1(x))
        x &#x3D; F.relu(self.fc2(x))
        x &#x3D; self.fc3(x)
        return x


model &#x3D; ConvNet().to(device)
criterion &#x3D; nn.CrossEntropyLoss()
optimizer &#x3D; torch.optim.SGD(model.parameters(), lr&#x3D;learning_rate)

n_total_steps &#x3D; len(train_loader)
for epoch in range(num_epochs):
    for i,(images, labels) in enumerate(train_loader):
        # origin shape:[4,3,32,32] &#x3D; 4,3, 1024
        # input_layer:3 input channels, 6 output channels, 5 kernel size
        images &#x3D; images.to(device)
        labels &#x3D; labels.to(device)

        # Forward pass
        outputs &#x3D; model(images)
        loss &#x3D; criterion(outputs, labels)

        # Backward and optimizer
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if (i+1) % 2000 &#x3D;&#x3D; 0:
            print(f&#39;Epoch [&#123;epoch+1&#125;&#x2F;&#123;num_epochs&#125;], Step [&#123;i+1&#125;&#x2F;&#123;n_total_steps&#125;], Loss: &#123;loss.item():.4f&#125;&#39;)

# test
with torch.no_grad():
    n_correct &#x3D; 0
    n_samples &#x3D; 0
    n_class_correct &#x3D; [0 for i in range(10)]
    n_class_samples &#x3D; [0 for i in range(10)]
    for images, labels in test_loader:
        images &#x3D; images.to(device)
        labels &#x3D; labels.to(device)
        outputs &#x3D; model(images)
        # max returns (value ,index)
        _, predicted &#x3D; torch.max(outputs, 1)
        n_samples +&#x3D; labels.size(0)
        n_correct +&#x3D; (predicted &#x3D;&#x3D; labels).sum().item()

        for i in range(batch_size):
            label &#x3D; labels[i]
            pred &#x3D; predicted[i]
            if (label &#x3D;&#x3D; pred):
                n_class_correct[label] +&#x3D; 1
            n_class_samples[label] +&#x3D; 1

    acc &#x3D; 100.0 * n_correct &#x2F; n_samples
    print(f&#39;Accuracy of the network: &#123;acc&#125; %&#39;)

    for i in range(10):
        acc &#x3D; 100.0 * n_class_correct[i] &#x2F; n_class_samples[i]
        print(f&#39;Accuracy of &#123;classes[i]&#125;: &#123;acc&#125; %&#39;)
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>


</blockquote>
<h3 id="10-Transfer-Learning"><a href="#10-Transfer-Learning" class="headerlink" title="10. Transfer Learning"></a>10. Transfer Learning</h3><blockquote>
<p>è¿™é‡Œæˆ‘ä»¬å­¦ä¹ è¿ç§»å­¦ä¹ ï¼Œä¸»è¦å­¦ä¹ å¦‚ä¸‹ä¸‰ä¸ªçŸ¥è¯†ç‚¹ï¼š</p>
<ol>
<li><p>ImageFolder: how we can use ImageFolder</p>
</li>
<li><p>Scheduler: how we use a scheduler to change the learning rate</p>
</li>
<li><p>Transfer Learning</p>
<p>è¿ç§»å­¦ä¹ æ˜¯æŒ‡åœ¨æŸä¸€ä¸ªä»»åŠ¡ä¸Šè®­ç»ƒå¥½æ¨¡å‹ï¼Œç„¶åå°†è®­ç»ƒå¥½çš„æ¨¡å‹è¿ç§»åˆ°å¦å¤–ä¸€ä¸ªä»»åŠ¡ä¸­ï¼Œå›ºå®šæ¨¡å‹çš„æŸäº›å‚æ•°ï¼Œå¯¹å¦å¤–ä¸€äº›å‚æ•°è¿›è¡Œå­¦ä¹ ï¼Œä»¥ä¾¿æ¨¡å‹èƒ½å¤Ÿé€‚åº”æ–°çš„ä»»åŠ¡ã€‚</p>
<p>å®éªŒä»£ç å¦‚ä¸‹ï¼š</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># Transfer Learning
                                 
# 1) ImageFolder: how we can use ImageFolder
# 2) Scheduler: how we use a scheduler to change the learning rate
# 3) Transfer Learning:
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim import lr_scheduler
import numpy as np
import torchvision
from torchvision import datasets, models, transforms
import matplotlib.pyplot as plt
import time
import os
import copy
                                 
# device config
device &#x3D;torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)
                                 
mean &#x3D; np.array([0.485, 0.456, 0.406])
std &#x3D; np.array([0.229, 0.224, 0.225])
                                 
data_transforms &#x3D; &#123;
    &#39;train&#39;:transforms.Compose([
        transforms.RandomResizedCrop(224),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize(mean, std)
    ]),
    &#39;val&#39;:transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(mean, std)
    ])
&#125;
                                 
# import data
data_dir &#x3D; &#39;data&#x2F;hymenoptera_data&#39;
sets &#x3D; [&#39;train&#39;, &#39;val&#39;]
image_datasets &#x3D; &#123;x:datasets.ImageFolder(os.path.join(data_dir, x),
                                         data_transforms[x])
                    for x in sets&#125;
dataloaders &#x3D; &#123;x: torch.utils.data.DataLoader(image_datasets[x], batch_size&#x3D;4,
                                            shuffle&#x3D;True, num_workers&#x3D;0)
               for x in sets&#125;
dataset_sizes &#x3D; &#123;x: len(image_datasets[x]) for x in sets&#125;
class_names &#x3D; image_datasets[&#39;train&#39;].classes
print(class_names)
                             
def imshow(inp, title):
    &quot;&quot;&quot;Imshow for Tensor.&quot;&quot;&quot;
    inp &#x3D; inp.numpy().transpose((1, 2, 0))
    inp &#x3D; std * inp + mean
    inp &#x3D; np.clip(inp, 0, 1)
    plt.imshow(inp)
    plt.title(title)
    plt.show()
                              
# Get a batch of training data
inputs, classes &#x3D; next(iter(dataloaders[&#39;train&#39;]))
                              
# Make a grid from batch
out &#x3D; torchvision.utils.make_grid(inputs)
                              
imshow(out, title&#x3D;[class_names[x] for x in classes])
                              
def train_model(model, criterion, optimizer, scheduler, num_epochs&#x3D;25):
    since &#x3D; time.time()
                              
    best_model_wts &#x3D; copy.deepcopy(model.state_dict())
    best_acc &#x3D; 0.0
                                  
    for epoch in range(num_epochs):
        print(f&#39;Epoch &#123;epoch+1&#125; &#x2F; &#123;num_epochs&#125;&#39;)
        print(&#39;-&#39;*15)
                                  
        # Each epoch has a training and validation phase
        for phase in sets:
            if phase &#x3D;&#x3D;&#39;train&#39;:
                model.train()
            else:
                model.eval()
                                  
            running_loss &#x3D; 0.0
            running_correct &#x3D; 0.0
                                  
            # Iterate over data.
            for inputs, labels in dataloaders[phase]:
                inputs &#x3D; inputs.to(device)
                labels &#x3D; labels.to(device)
                                  
                # forward
                # track history if only training
                with torch.set_grad_enabled(phase &#x3D;&#x3D; &#39;train&#39;):
                    outputs &#x3D; model(inputs)
                    loss &#x3D; criterion(outputs, labels)
                    _, preds &#x3D; torch.max(outputs, 1)
                                  
                    # backward + optimizer only if in training phase
                    if phase &#x3D;&#x3D; &#39;train&#39;:
                        optimizer.zero_grad()
                        loss.backward()
                        optimizer.step()
                                  
                # statistics
                running_loss +&#x3D; loss.item()*inputs.size(0)
                running_correct +&#x3D; torch.sum(preds &#x3D;&#x3D; labels.data)
                                  
            if phase &#x3D;&#x3D; &#39;train&#39;:
                scheduler.step()
                                  
            epoch_loss &#x3D; running_loss &#x2F; dataset_sizes[phase]
            epoch_acc &#x3D; running_correct.double() &#x2F; dataset_sizes[phase]
                                  
            print(f&#39;&#123;phase&#125; Loss: &#123;epoch_loss:.4f&#125; Acc: &#123;epoch_acc:.4f&#125;&#39;)
                                  
            # deep copy the model to
            if phase &#x3D;&#x3D; &#39;val&#39; and epoch_acc &gt; best_acc:
                best_acc &#x3D; epoch_acc
                best_model_wts &#x3D; copy.deepcopy(model.state_dict())
                                  
    time_elapsed &#x3D; time.time()-since
    print(f&#39;Training complete in &#123;time_elapsed&#x2F;&#x2F;60:.0f&#125;m &#123;time_elapsed%60:.0fs&#125;&#39;)
    print(f&#39;Best val Acc: &#123;best_acc:4f&#125;&#39;)
                                  
    #load best model weights
    model.load_state_dic(best_model_wts)
    return model
                              
model &#x3D; models.resnet18(pretrained&#x3D;True)
                              
# Here, we need to freeze all the network except the final layer.
# We need to set requires_grad &#x3D;&#x3D; False to freeze the parameters so that the gradients are not computed in backward()
for param in model.parameters():
    param.requires_grad &#x3D; False
                              
num_ftrs &#x3D; model.fc.in_features
                              
model.fc &#x3D; nn.Linear(num_ftrs, 2)
model.to(device)
                              
criterion &#x3D; nn.CrossEntropyLoss()
                              
# Observe that all parameters are being optimized
optimizer &#x3D; optim.SGD(model.parameters(), lr&#x3D;0.001)
                              
# scheduler
step_lr_schedule &#x3D; lr_scheduler.StepLR(optimizer, step_size&#x3D;7, gamma&#x3D;0.1) # Every 7 stepï¼Œ learning rate multiple by 0.1
                              
model &#x3D; train_model(model, criterion, optimizer, step_lr_schedule, num_epochs&#x3D;7)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li>
</ol>
</blockquote>
<h3 id="11-Tensorboard"><a href="#11-Tensorboard" class="headerlink" title="11. Tensorboard"></a>11. Tensorboard</h3><blockquote>
<p>Tensorbardæ˜¯å¾ˆå¥½çš„å¯è§†åŒ–å·¥å…·ï¼Œå…·ä½“å¯ä»¥å‚è€ƒå¦‚ä¸‹çš„ä»£ç ã€‚</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># Tensorboard

import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt

############## TENSORBOARD ########################
import sys
import torch.nn.functional as F
from torch.utils.tensorboard import SummaryWriter

# default &#96;log_dir&#96; is &quot;runs&quot; - we&#39;ll be more specific here
writer &#x3D; SummaryWriter(&#39;runs&#x2F;mnist1&#39;)
###################################################

# Device configuration
device &#x3D; torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)

# Hyper-parameters
input_size &#x3D; 784  # 28x28
hidden_size &#x3D; 500
num_classes &#x3D; 10
num_epochs &#x3D; 1
batch_size &#x3D; 64
learning_rate &#x3D; 0.001

# MNIST dataset
train_dataset &#x3D; torchvision.datasets.MNIST(root&#x3D;&#39;.&#x2F;data&#39;,
                                           train&#x3D;True,
                                           transform&#x3D;transforms.ToTensor(),
                                           download&#x3D;True)

test_dataset &#x3D; torchvision.datasets.MNIST(root&#x3D;&#39;.&#x2F;data&#39;,
                                          train&#x3D;False,
                                          transform&#x3D;transforms.ToTensor())

# Data loader
train_loader &#x3D; torch.utils.data.DataLoader(dataset&#x3D;train_dataset,
                                           batch_size&#x3D;batch_size,
                                           shuffle&#x3D;True)

test_loader &#x3D; torch.utils.data.DataLoader(dataset&#x3D;test_dataset,
                                          batch_size&#x3D;batch_size,
                                          shuffle&#x3D;False)

examples &#x3D; iter(test_loader)
example_data, example_targets &#x3D; examples.next()

for i in range(6):
    plt.subplot(2, 3, i + 1)
    plt.imshow(example_data[i][0], cmap&#x3D;&#39;gray&#39;)
# plt.show()

############## TENSORBOARD ########################
img_grid &#x3D; torchvision.utils.make_grid(example_data)
writer.add_image(&#39;mnist_images&#39;, img_grid)


# writer.close()
# sys.exit()
###################################################

# Fully connected neural network with one hidden layer
class NeuralNet(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super(NeuralNet, self).__init__()
        self.input_size &#x3D; input_size
        self.l1 &#x3D; nn.Linear(input_size, hidden_size)
        self.relu &#x3D; nn.ReLU()
        self.l2 &#x3D; nn.Linear(hidden_size, num_classes)

    def forward(self, x):
        out &#x3D; self.l1(x)
        out &#x3D; self.relu(out)
        out &#x3D; self.l2(out)
        # no activation and no softmax at the end
        return out


model &#x3D; NeuralNet(input_size, hidden_size, num_classes).to(device)

# Loss and optimizer
criterion &#x3D; nn.CrossEntropyLoss()
optimizer &#x3D; torch.optim.Adam(model.parameters(), lr&#x3D;learning_rate)

############## TENSORBOARD ########################
writer.add_graph(model, example_data.reshape(-1, 28 * 28))
# writer.close()
# sys.exit()
###################################################

# Train the model
running_loss &#x3D; 0.0
running_correct &#x3D; 0
n_total_steps &#x3D; len(train_loader)
for epoch in range(num_epochs):
    for i, (images, labels) in enumerate(train_loader):
        # origin shape: [100, 1, 28, 28]
        # resized: [100, 784]
        images &#x3D; images.reshape(-1, 28 * 28).to(device)
        labels &#x3D; labels.to(device)

        # Forward pass
        outputs &#x3D; model(images)
        loss &#x3D; criterion(outputs, labels)

        # Backward and optimize
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        running_loss +&#x3D; loss.item()

        _, predicted &#x3D; torch.max(outputs.data, 1)
        running_correct +&#x3D; (predicted &#x3D;&#x3D; labels).sum().item()
        if (i + 1) % 100 &#x3D;&#x3D; 0:
            print(f&#39;Epoch [&#123;epoch + 1&#125;&#x2F;&#123;num_epochs&#125;], Step [&#123;i + 1&#125;&#x2F;&#123;n_total_steps&#125;], Loss: &#123;loss.item():.4f&#125;&#39;)
            ############## TENSORBOARD ########################
            writer.add_scalar(&#39;training loss&#39;, running_loss &#x2F; 100, epoch * n_total_steps + i)
            running_accuracy &#x3D; running_correct &#x2F; 100 &#x2F; predicted.size(0)
            writer.add_scalar(&#39;accuracy&#39;, running_accuracy, epoch * n_total_steps + i)
            running_correct &#x3D; 0
            running_loss &#x3D; 0.0
            ###################################################

# Test the model
# In test phase, we don&#39;t need to compute gradients (for memory efficiency)
class_labels &#x3D; []
class_preds &#x3D; []
with torch.no_grad():
    n_correct &#x3D; 0
    n_samples &#x3D; 0
    for images, labels in test_loader:
        images &#x3D; images.reshape(-1, 28 * 28).to(device)
        labels &#x3D; labels.to(device)
        outputs &#x3D; model(images)
        # max returns (value ,index)
        values, predicted &#x3D; torch.max(outputs.data, 1)
        n_samples +&#x3D; labels.size(0)
        n_correct +&#x3D; (predicted &#x3D;&#x3D; labels).sum().item()

        class_probs_batch &#x3D; [F.softmax(output, dim&#x3D;0) for output in outputs]

        class_preds.append(class_probs_batch)
        class_labels.append(predicted)

    # 10000, 10, and 10000, 1
    # stack concatenates tensors along a new dimension
    # cat concatenates tensors in the given dimension
    class_preds &#x3D; torch.cat([torch.stack(batch) for batch in class_preds])
    class_labels &#x3D; torch.cat(class_labels)

    acc &#x3D; 100.0 * n_correct &#x2F; n_samples
    print(f&#39;Accuracy of the network on the 10000 test images: &#123;acc&#125; %&#39;)

    ############## TENSORBOARD ########################
    classes &#x3D; range(10)
    for i in classes:
        labels_i &#x3D; class_labels &#x3D;&#x3D; i
        preds_i &#x3D; class_preds[:, i]
        writer.add_pr_curve(str(i), labels_i, preds_i, global_step&#x3D;0)
        writer.close()
    ###################################################<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>


</blockquote>
<h3 id="12-Saving-and-Loading-model"><a href="#12-Saving-and-Loading-model" class="headerlink" title="12. Saving and Loading model"></a>12. Saving and Loading model</h3><blockquote>
 <pre class="line-numbers language-none"><code class="language-none">
2 DIFFERENT WAYS OF SAVING
# 1) lazy way: save whole model
torch.save(model, PATH)

# model class must be defined somewhere
model &#x3D; torch.load(PATH)
model.eval()

# 2) recommended way: save only the state_dict
torch.save(model.state_dict(), PATH)

# model must be created again with parameters
model &#x3D; Model(*args, **kwargs)
model.load_state_dict(torch.load(PATH))
model.eval()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p> ç¤ºä¾‹ä»£ç å¦‚ä¸‹:</p>
 <pre class="line-numbers language-python" data-language="python"><code class="language-python">import torch
import torch.nn as nn

class Model(nn.Module):
def __init__(self, n_input_features):
    super(Model, self).__init__()
    self.linear &#x3D; nn.Linear(n_input_features, 1)

def forward(self, x):
    y_pred &#x3D; torch.sigmoid(self.linear(x))
    return y_pred

model &#x3D; Model(n_input_features&#x3D;6)
# train your model...

####################save all ######################################
for param in model.parameters():
print(param)

# save and load entire model

FILE &#x3D; &quot;model.pth&quot;
torch.save(model, FILE)

loaded_model &#x3D; torch.load(FILE)
loaded_model.eval()

for param in loaded_model.parameters():
print(param)


############save only state dict #########################

# save only state dict
FILE &#x3D; &quot;model.pth&quot;
torch.save(model.state_dict(), FILE)

print(model.state_dict())
loaded_model &#x3D; Model(n_input_features&#x3D;6)
loaded_model.load_state_dict(torch.load(FILE)) # it takes the loaded dictionary, not the path file itself
loaded_model.eval()

print(loaded_model.state_dict())


###########load checkpoint#####################
learning_rate &#x3D; 0.01
optimizer &#x3D; torch.optim.SGD(model.parameters(), lr&#x3D;learning_rate)

checkpoint &#x3D; &#123;
&quot;epoch&quot;: 90,
&quot;model_state&quot;: model.state_dict(),
&quot;optim_state&quot;: optimizer.state_dict()
&#125;
print(optimizer.state_dict())
FILE &#x3D; &quot;checkpoint.pth&quot;
torch.save(checkpoint, FILE)

model &#x3D; Model(n_input_features&#x3D;6)
optimizer &#x3D; optimizer &#x3D; torch.optim.SGD(model.parameters(), lr&#x3D;0)

checkpoint &#x3D; torch.load(FILE)
model.load_state_dict(checkpoint[&#39;model_state&#39;])
optimizer.load_state_dict(checkpoint[&#39;optim_state&#39;])
epoch &#x3D; checkpoint[&#39;epoch&#39;]

model.eval()
# - or -
# model.train()

print(optimizer.state_dict())

# Remember that you must call model.eval() to set dropout and batch normalization layers 
# to evaluation mode before running inference. Failing to do this will yield 
# inconsistent inference results. If you wish to resuming training, 
# call model.train() to ensure these layers are in training mode.

&quot;&quot;&quot; SAVING ON GPU&#x2F;CPU 

# 1) Save on GPU, Load on CPU
device &#x3D; torch.device(&quot;cuda&quot;)
model.to(device)
torch.save(model.state_dict(), PATH)

device &#x3D; torch.device(&#39;cpu&#39;)
model &#x3D; Model(*args, **kwargs)
model.load_state_dict(torch.load(PATH, map_location&#x3D;device))

# 2) Save on GPU, Load on GPU
device &#x3D; torch.device(&quot;cuda&quot;)
model.to(device)
torch.save(model.state_dict(), PATH)

model &#x3D; Model(*args, **kwargs)
model.load_state_dict(torch.load(PATH))
model.to(device)

# Note: Be sure to use the .to(torch.device(&#39;cuda&#39;)) function 
# on all model inputs, too!

# 3) Save on CPU, Load on GPU
torch.save(model.state_dict(), PATH)

device &#x3D; torch.device(&quot;cuda&quot;)
model &#x3D; Model(*args, **kwargs)
model.load_state_dict(torch.load(PATH, map_location&#x3D;&quot;cuda:0&quot;))  # Choose whatever GPU device number you want
model.to(device)

# This loads the model to a given GPU device. 
# Next, be sure to call model.to(torch.device(&#39;cuda&#39;)) to convert the modelâ€™s parameter tensors to CUDA tensors
&quot;&quot;&quot;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
</blockquote>
<h3 id="13-Summary"><a href="#13-Summary" class="headerlink" title="13. Summary"></a>13. Summary</h3><img src="https://raw.githubusercontent.com/Skylyong/i/main/20210805230250.png" alt="pytorch" style="zoom:50%;" />


    </article>
    <!-- license -->
        <div class="license-wrapper">
            <p>åŸæ–‡ä½œè€…ï¼š<a href="https://ailyong.cn">ğŸ†</a>
            <p>åŸæ–‡é“¾æ¥ï¼š<a href="https://ailyong.cn/2021/08/05/pytorch-xue-xi/">https://ailyong.cn/2021/08/05/pytorch-xue-xi/</a>
            <p>å‘è¡¨æ—¥æœŸï¼š<a href="https://ailyong.cn/2021/08/05/pytorch-xue-xi/">August 5th 2021, 9:50:37 pm</a>
            <p>æ›´æ–°æ—¥æœŸï¼š<a href="https://ailyong.cn/2021/08/05/pytorch-xue-xi/">August 5th 2021, 11:04:48 pm</a>
            <p>ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡é‡‡ç”¨<a rel="license noopener" target="_blank" href="http://creativecommons.org/licenses/by-nc/4.0/">çŸ¥è¯†å…±äº«ç½²å-éå•†ä¸šæ€§ä½¿ç”¨ 4.0 å›½é™…è®¸å¯åè®®</a>è¿›è¡Œè®¸å¯</p>
        </div>
    <!-- paginator -->
    <ul class="post-paginator">
        <li class="next">
                <div class="nextSlogan">Next Post</div>
                <a href="/2021/08/06/pandas-xue-xi/" title="Pandaså­¦ä¹ ">
                    <div class="nextTitle">Pandaså­¦ä¹ </div>
                </a>
        </li>
        <li class="previous">
                <div class="prevSlogan">Previous Post</div>
                <a href="/2021/07/20/li-bao-chun-writing-perfect-papers/" title="">
                    <div class="prevTitle">[Untitled Post]</div>
                </a>
        </li>
    </ul>
    <!-- comment -->
        <div class="post-comment">
            <!-- æ¥å¿…åŠ› City ç‰ˆå®‰è£…ä»£ç  -->

            
            
            
            <!-- utterancè¯„è®º -->

            <!-- partial('_partial/comment/changyan') -->
            <!--PCç‰ˆ-->

            
            
            
        </div>
    <!-- timeliness note -->
    <!-- idea from: https://hexo.fluid-dev.com/posts/hexo-injector/#%E6%96%87%E7%AB%A0%E6%97%B6%E6%95%88%E6%80%A7%E6%8F%90%E7%A4%BA -->
    <!-- Mathjax -->
</main>

                <!-- profile -->
            </div>
            <footer class="footer footer-unloaded">
    <!-- social  -->
        <div class="social">
                            <a href="mailto:lyong_s@foxmail.com" class="iconfont-archer email" title="email" ></a>
                <a href="//github.com/Skylyong" class="iconfont-archer github" target="_blank" title="github"></a>

        </div>
    <!-- powered by Hexo  -->
    <div class="copyright">
        <span id="hexo-power">Powered by <a href="https://hexo.io/" target="_blank">Hexo</a></span><span class="iconfont-archer power">&#xe635;</span><span id="theme-info">theme <a href="https://github.com/fi3ework/hexo-theme-archer" target="_blank">Archer</a></span>
    </div>
    <!-- website approve for Chinese user -->
    <!-- ä¸è’œå­  -->
        <div class="busuanzi-container">
                <span id="busuanzi_container_site_pv">PV: <span id="busuanzi_value_site_pv"></span> :)</span>
        </div>
</footer>

        </div>
        <!-- toc -->
            <div class="toc-wrapper toc-wrapper-loding" style=    top:50vh;
>
                <div class="toc-catalog">
                    <span class="iconfont-archer catalog-icon">&#xe613;</span><span>CATALOG</span>
                </div>
                <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Tensor"><span class="toc-number">1.</span> <span class="toc-text">1. Tensor</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-How-to-create-tensor"><span class="toc-number">1.1.</span> <span class="toc-text">1.1 How to create tensor</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-%E5%AF%B9Tensor%E7%9A%84%E5%90%84%E7%A7%8D%E6%93%8D%E4%BD%9C"><span class="toc-number">1.2.</span> <span class="toc-text">1.2 å¯¹Tensorçš„å„ç§æ“ä½œ</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-%E5%9F%BA%E6%9C%AC%E8%BF%90%E7%AE%97"><span class="toc-number">1.3.</span> <span class="toc-text">1.3 åŸºæœ¬è¿ç®—</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Autograd"><span class="toc-number">2.</span> <span class="toc-text">2. Autograd</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-Calculate-the-gradients"><span class="toc-number">2.1.</span> <span class="toc-text">2.1 Calculate the gradients</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-How-to-prevent-pytorch-from-tracking-the-history-and-calculating-this-grad-fn-attribute"><span class="toc-number">2.2.</span> <span class="toc-text">2.2 How to prevent pytorch from tracking the history and calculating this grad fn attribute.</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-%E6%A2%AF%E5%BA%A6%E7%B4%AF%E7%A7%AF"><span class="toc-number">2.3.</span> <span class="toc-text">2.3 æ¢¯åº¦ç´¯ç§¯</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-Backpropagation"><span class="toc-number">3.</span> <span class="toc-text">3. Backpropagation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-Optimize-model-with-automatic-gradient-computation"><span class="toc-number">4.</span> <span class="toc-text">4. Optimize model with automatic gradient computation</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-%E7%94%A8numpy%E5%AE%9E%E7%8E%B0%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95"><span class="toc-number">4.1.</span> <span class="toc-text">4.1 ç”¨numpyå®ç°å›å½’ç®—æ³•</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-%E7%94%A8pytorch%E6%9B%BF%E6%8D%A2%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E5%92%8C%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97"><span class="toc-number">4.2.</span> <span class="toc-text">4.2 ç”¨pytorchæ›¿æ¢æ•°æ®ç±»å‹å’Œæ¢¯åº¦è®¡ç®—</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-%E7%94%A8pytorch%E6%9D%A5%E5%81%9A%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97%E5%92%8C%E4%BC%98%E5%8C%96"><span class="toc-number">4.3.</span> <span class="toc-text">4.3 ç”¨pytorchæ¥åšæ¢¯åº¦è®¡ç®—å’Œä¼˜åŒ–</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-4-%E5%AE%9A%E4%B9%89%E6%9B%B4%E5%8A%A0%E5%A4%8D%E6%9D%82%E7%9A%84%E6%A8%A1%E5%9E%8B"><span class="toc-number">4.4.</span> <span class="toc-text">4.4 å®šä¹‰æ›´åŠ å¤æ‚çš„æ¨¡å‹</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-5-%E7%BB%BC%E5%90%88%E7%A4%BA%E4%BE%8B1"><span class="toc-number">4.5.</span> <span class="toc-text">4.5 ç»¼åˆç¤ºä¾‹1</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-6-%E7%BB%BC%E5%90%88%E7%A4%BA%E4%BE%8B2"><span class="toc-number">4.6.</span> <span class="toc-text">4.6  ç»¼åˆç¤ºä¾‹2</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-Dataset-and-Dataload-Class"><span class="toc-number">5.</span> <span class="toc-text">5. Dataset and Dataload Class</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#5-1-Dataset-and-Dataload"><span class="toc-number">5.1.</span> <span class="toc-text">5.1 Dataset and Dataload</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-2-Transforms-for-the-dataset"><span class="toc-number">5.2.</span> <span class="toc-text">5.2 Transforms for the dataset</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-Softmax-and-Cross-Entropy"><span class="toc-number">6.</span> <span class="toc-text">6. Softmax and Cross-Entropy</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#6-1-Softmax"><span class="toc-number">6.1.</span> <span class="toc-text">6.1 Softmax</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-2-Cross-entropy"><span class="toc-number">6.2.</span> <span class="toc-text">6.2 Cross entropy</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-Activation-function"><span class="toc-number">7.</span> <span class="toc-text">7. Activation function</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Most-popular-activation-functions"><span class="toc-number">7.1.</span> <span class="toc-text">Most popular activation functions</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-%E7%BB%BC%E5%90%88%E7%BB%83%E4%B9%A0-MNIST"><span class="toc-number">8.</span> <span class="toc-text">8. ç»¼åˆç»ƒä¹  MNIST</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-Convolutional-Neural-Net-CNN"><span class="toc-number">9.</span> <span class="toc-text">9.  Convolutional Neural Net(CNN)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-Transfer-Learning"><span class="toc-number">10.</span> <span class="toc-text">10. Transfer Learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-Tensorboard"><span class="toc-number">11.</span> <span class="toc-text">11. Tensorboard</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-Saving-and-Loading-model"><span class="toc-number">12.</span> <span class="toc-text">12. Saving and Loading model</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-Summary"><span class="toc-number">13.</span> <span class="toc-text">13. Summary</span></a></li></ol>
            </div>
        <!-- sidebar -->
        <div class="sidebar sidebar-hide">
    <ul class="sidebar-tabs sidebar-tabs-active-0">
        <li class="sidebar-tab-archives"><span class="iconfont-archer">&#xe67d;</span><span class="tab-name">Archive</span></li>
        <li class="sidebar-tab-tags"><span class="iconfont-archer">&#xe61b;</span><span class="tab-name">Tag</span></li>
        <li class="sidebar-tab-categories"><span class="iconfont-archer">&#xe666;</span><span class="tab-name">Cate</span></li>
    </ul>
    <div class="sidebar-content sidebar-content-show-archive">
        <div class="sidebar-panel-archives">
    <!-- åœ¨ ejs ä¸­å°† archive æŒ‰ç…§æ—¶é—´æ’åº -->
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
    <div class="total-and-search">
        <div class="total-archive">
        Total : 42
        </div>
        <!-- search  -->
    </div>
    <div class="post-archive">
            <div class="archive-year"> 2025 </div>
            <ul class="year-list">
        <li class="archive-post-item">
            <span class="archive-post-date">01/28</span>
            <a class="archive-post-title" href="/2025/01/28/bo-ke-wei-hu-ri-zhi/">åšå®¢ç»´æŠ¤æ—¥å¿—.md</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">01/27</span>
            <a class="archive-post-title" href="/2025/01/27/2024-nian-di-zong-jie/">2024å¹´åº•æ€»ç»“</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">01/26</span>
            <a class="archive-post-title" href="/2025/01/26/da-jian-webdav-fu-wu-qi-shi-xian-obsidian-he-zotero-duo-duan-tong-bu/">æ­å»ºWebdavæœåŠ¡å™¨å®ç°Obsidianå’ŒZoteroå¤šç«¯åŒæ­¥</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">01/07</span>
            <a class="archive-post-title" href="/2025/01/07/sentence-transformers-sampler-bug-zhi-set-bu-yi-ding-shi-wu-xu-de/">sentence-transformers sampler bug ä¹‹ set ä¸ä¸€å®šæ˜¯æ— åºçš„</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">01/06</span>
            <a class="archive-post-title" href="/2025/01/06/matplotlib-xian-shi-zhong-wen/">matplotlib è®¾ç½®æ˜¾ç¤ºä¸­æ–‡</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">01/06</span>
            <a class="archive-post-title" href="/2025/01/06/python-duo-jin-cheng-shi-li/">pythonå¤šè¿›ç¨‹ç¤ºä¾‹</a>
        </li>
                </ul>
            <div class="archive-year"> 2023 </div>
            <ul class="year-list">
        <li class="archive-post-item">
            <span class="archive-post-date">09/12</span>
            <a class="archive-post-title" href="/2023/09/12/ubuntu-shang-an-zhuang-nividia-qu-dong-he-cuda-tookit/">ubuntuä¸Šå®‰è£…NIVIDIAé©±åŠ¨å’ŒCUDA_Tookit</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">07/23</span>
            <a class="archive-post-title" href="/2023/07/23/zai-mac-shang-shi-yong-pybind11-shi-xian-c-yu-python-hu-tong-de-demo/">åœ¨macä¸Šä½¿ç”¨pybind11å®ç°c++ä¸pythonäº’é€šçš„demo</a>
        </li>
                </ul>
            <div class="archive-year"> 2022 </div>
            <ul class="year-list">
        <li class="archive-post-item">
            <span class="archive-post-date">09/07</span>
            <a class="archive-post-title" href="/2022/09/07/mac-xi-tong-zhi-zuo-win10-qi-dong-u-pan/">Macç³»ç»Ÿåˆ¶ä½œwin10å¯åŠ¨Uç›˜</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">09/05</span>
            <a class="archive-post-title" href="/2022/09/05/dian-ci-bi-sai-zong-jie/">ç”µç£æ¯”èµ›æ€»ç»“</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">04/29</span>
            <a class="archive-post-title" href="/2022/04/29/yuan-cheng-fang-wen-fu-wu-qi-jupyter-notebook/">è¿œç¨‹è®¿é—®æœåŠ¡å™¨Jupyter-Notebook</a>
        </li>
                </ul>
            <div class="archive-year"> 2021 </div>
            <ul class="year-list">
        <li class="archive-post-item">
            <span class="archive-post-date">12/10</span>
            <a class="archive-post-title" href="/2021/12/10/c-duo-xian-cheng-bian-cheng/">C++å¤šçº¿ç¨‹ç¼–ç¨‹</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">09/29</span>
            <a class="archive-post-title" href="/2021/09/29/jiang-wei-xin-liao-tian-zhong-de-yu-yin-zhuan-huan-wei-mp3/">å°†å¾®ä¿¡èŠå¤©ä¸­çš„è¯­éŸ³è½¬æ¢ä¸ºmp3</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">09/19</span>
            <a class="archive-post-title" href="/2021/09/19/you-yong-de-zi-yuan/">æœ‰ç”¨çš„èµ„æº</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">09/19</span>
            <a class="archive-post-title" href="/2021/09/19/ji-qi-xue-xi-can-kao-zi-liao/">æœºå™¨å­¦ä¹ å‚è€ƒèµ„æ–™</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">08/14</span>
            <a class="archive-post-title" href="/2021/08/14/wei-xin-biao-qian-zheng-li/">å¾®ä¿¡æ ‡ç­¾æ•´ç†</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">08/14</span>
            <a class="archive-post-title" href="/2021/08/14/gong-ju-xiang/">å·¥å…·ç®±</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">08/13</span>
            <a class="archive-post-title" href="/2021/08/13/jiao-shi-zi-ge-zheng-kao-shi-xue-xi-bi-ji/">æ•™å¸ˆèµ„æ ¼è¯è€ƒè¯•å­¦ä¹ ç¬”è®°</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">08/13</span>
            <a class="archive-post-title" href="/2021/08/13/google-fang-wen-zhu-shou-an-zhuang/">Googleè®¿é—®åŠ©æ‰‹å®‰è£…</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">08/13</span>
            <a class="archive-post-title" href="/2021/08/13/tu-de-chuang-jian-he-bian-li/">ã€è¯¾ç¨‹è®¾è®¡ã€‘å›¾çš„åˆ›å»ºå’Œéå†</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">08/13</span>
            <a class="archive-post-title" href="/2021/08/13/chang-yong-shu-ju-ji-zheng-li/">å¸¸ç”¨æ•°æ®é›†æ•´ç†</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">08/09</span>
            <a class="archive-post-title" href="/2021/08/09/svm-xue-xi-bi-ji/">SVMå­¦ä¹ ç¬”è®°</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">08/06</span>
            <a class="archive-post-title" href="/2021/08/06/pandas-xue-xi/">Pandaså­¦ä¹ </a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">08/05</span>
            <a class="archive-post-title" href="/2021/08/05/pytorch-xue-xi/">Pytorchå­¦ä¹ </a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">07/20</span>
            <a class="archive-post-title" href="/2021/07/20/li-bao-chun-writing-perfect-papers/">[Untitled Post]</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">07/01</span>
            <a class="archive-post-title" href="/2021/07/01/huggingface-yu-xun-lian-mo-xing-quan-chong-xia-zai-de-wen-ti/">Huggingface é¢„è®­ç»ƒæ¨¡å‹æƒé‡ä¸‹è½½çš„é—®é¢˜</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">06/21</span>
            <a class="archive-post-title" href="/2021/06/21/jupyter-notebook-ben-di-fu-wu-qi-da-jian/">jupyter notebook æœ¬åœ°æœåŠ¡å™¨æ­å»º</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">04/16</span>
            <a class="archive-post-title" href="/2021/04/16/ji-qi-xue-xi-zhong-de-ge-chong-shang/">æœºå™¨å­¦ä¹ ä¸­çš„å„ç§ç†µ</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">04/08</span>
            <a class="archive-post-title" href="/2021/04/08/git-chang-yong-ming-ling/">gitå¸¸ç”¨å‘½ä»¤</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">04/08</span>
            <a class="archive-post-title" href="/2021/04/08/nlp-zhi-shi-ti-xi/">NLPçŸ¥è¯†ä½“ç³»</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">04/01</span>
            <a class="archive-post-title" href="/2021/04/01/zui-xiao-lu-jing-he/">ã€æ¯æ—¥ç®—æ³•ã€‘2021å¹´03æœˆ31æ—¥ å°é›¨ æœ€å°è·¯å¾„å’Œ</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">03/31</span>
            <a class="archive-post-title" href="/2021/03/31/gitlab-da-jian-bi-ji/">GitLabæ­å»ºç¬”è®°</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">03/31</span>
            <a class="archive-post-title" href="/2021/03/31/ke-cheng-she-ji-zui-xiao-sheng-cheng-shu-ying-yong/">ã€è¯¾ç¨‹è®¾è®¡ã€‘æœ€å°ç”Ÿæˆæ ‘åº”ç”¨</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">03/31</span>
            <a class="archive-post-title" href="/2021/03/31/xue-xi-shi-pin-zheng-li/">å­¦ä¹ è§†é¢‘æ•´ç†</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">03/31</span>
            <a class="archive-post-title" href="/2021/03/31/ying-yu-kou-yu-lian-xi-dian-ying-tui-jian/">è‹±è¯­å£è¯­ç»ƒä¹ ç”µå½±æ¨è</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">03/30</span>
            <a class="archive-post-title" href="/2021/03/30/bu-tong-lu-jing/">ã€æ¯æ—¥ç®—æ³•ã€‘2021å¹´03æœˆ30æ—¥ å¤šäº‘ ä¸åŒè·¯å¾„</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">03/29</span>
            <a class="archive-post-title" href="/2021/03/29/bian-fen-xue-xi-bi-ji/">å˜åˆ†å­¦ä¹ ç¬”è®°</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">03/28</span>
            <a class="archive-post-title" href="/2021/03/28/ji-yi-ci-xi-tong-ben-kui-de-fan-si-he-zong-jie/">è®°ä¸€æ¬¡ç³»ç»Ÿå¥”æºƒçš„åæ€å’Œæ€»ç»“</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">03/28</span>
            <a class="archive-post-title" href="/2021/03/28/github-jia-su-fang-fa/">GitHubåŠ é€Ÿæ–¹æ³•</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">02/22</span>
            <a class="archive-post-title" href="/2021/02/22/ecnu-de-chun/">ECNUçš„æ˜¥</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">02/21</span>
            <a class="archive-post-title" href="/2021/02/21/build/">hexoä¸ªäººåšå®¢æ­å»º</a>
        </li>
        <li class="archive-post-item">
            <span class="archive-post-date">02/21</span>
            <a class="archive-post-title" href="/2021/02/21/hao-yong-de-mian-fei-wu-guang-gao-dian-ying-wang-zhan-tui-jian/">å¥½ç”¨çš„å…è´¹æ— å¹¿å‘Šç”µå½±ç½‘ç«™æ¨è</a>
        </li>
            </ul>
    </div>
</div>

        <div class="sidebar-panel-tags">
    <div class="sidebar-tags-name">
            <span class="sidebar-tag-name" data-tags="æ€»ç»“">
                <span class="iconfont-archer">&#xe606;</span>
                æ€»ç»“
            </span>
            <span class="sidebar-tag-name" data-tags="GitHub">
                <span class="iconfont-archer">&#xe606;</span>
                GitHub
            </span>
            <span class="sidebar-tag-name" data-tags="GitLab">
                <span class="iconfont-archer">&#xe606;</span>
                GitLab
            </span>
            <span class="sidebar-tag-name" data-tags="Googleè®¿é—®åŠ©æ‰‹">
                <span class="iconfont-archer">&#xe606;</span>
                Googleè®¿é—®åŠ©æ‰‹
            </span>
            <span class="sidebar-tag-name" data-tags="é¢„è®­ç»ƒæ¨¡å‹">
                <span class="iconfont-archer">&#xe606;</span>
                é¢„è®­ç»ƒæ¨¡å‹
            </span>
            <span class="sidebar-tag-name" data-tags="å¯åŠ¨Uç›˜">
                <span class="iconfont-archer">&#xe606;</span>
                å¯åŠ¨Uç›˜
            </span>
            <span class="sidebar-tag-name" data-tags="nlp">
                <span class="iconfont-archer">&#xe606;</span>
                nlp
            </span>
            <span class="sidebar-tag-name" data-tags="Pandas">
                <span class="iconfont-archer">&#xe606;</span>
                Pandas
            </span>
            <span class="sidebar-tag-name" data-tags="åšå®¢">
                <span class="iconfont-archer">&#xe606;</span>
                åšå®¢
            </span>
            <span class="sidebar-tag-name" data-tags="ECNU">
                <span class="iconfont-archer">&#xe606;</span>
                ECNU
            </span>
            <span class="sidebar-tag-name" data-tags="git">
                <span class="iconfont-archer">&#xe606;</span>
                git
            </span>
            <span class="sidebar-tag-name" data-tags="jupyter notebook">
                <span class="iconfont-archer">&#xe606;</span>
                jupyter notebook
            </span>
            <span class="sidebar-tag-name" data-tags="python">
                <span class="iconfont-archer">&#xe606;</span>
                python
            </span>
            <span class="sidebar-tag-name" data-tags="NIVIDIAé©±åŠ¨">
                <span class="iconfont-archer">&#xe606;</span>
                NIVIDIAé©±åŠ¨
            </span>
            <span class="sidebar-tag-name" data-tags="æ ‘">
                <span class="iconfont-archer">&#xe606;</span>
                æ ‘
            </span>
            <span class="sidebar-tag-name" data-tags="åŠ¨æ€è§„åˆ’">
                <span class="iconfont-archer">&#xe606;</span>
                åŠ¨æ€è§„åˆ’
            </span>
            <span class="sidebar-tag-name" data-tags="æœºå™¨å­¦ä¹ ">
                <span class="iconfont-archer">&#xe606;</span>
                æœºå™¨å­¦ä¹ 
            </span>
            <span class="sidebar-tag-name" data-tags="å›¾">
                <span class="iconfont-archer">&#xe606;</span>
                å›¾
            </span>
            <span class="sidebar-tag-name" data-tags="python_c++_äº’é€š">
                <span class="iconfont-archer">&#xe606;</span>
                python_c++_äº’é€š
            </span>
            <span class="sidebar-tag-name" data-tags="ç”µå½±">
                <span class="iconfont-archer">&#xe606;</span>
                ç”µå½±
            </span>
            <span class="sidebar-tag-name" data-tags="ç¼–ç¨‹">
                <span class="iconfont-archer">&#xe606;</span>
                ç¼–ç¨‹
            </span>
            <span class="sidebar-tag-name" data-tags="å¾®ä¿¡">
                <span class="iconfont-archer">&#xe606;</span>
                å¾®ä¿¡
            </span>
            <span class="sidebar-tag-name" data-tags="å¹²è´§">
                <span class="iconfont-archer">&#xe606;</span>
                å¹²è´§
            </span>
            <span class="sidebar-tag-name" data-tags="æ•™å¸ˆèµ„æ ¼è¯">
                <span class="iconfont-archer">&#xe606;</span>
                æ•™å¸ˆèµ„æ ¼è¯
            </span>
            <span class="sidebar-tag-name" data-tags="èµ„æº">
                <span class="iconfont-archer">&#xe606;</span>
                èµ„æº
            </span>
            <span class="sidebar-tag-name" data-tags="ç†µ">
                <span class="iconfont-archer">&#xe606;</span>
                ç†µ
            </span>
            <span class="sidebar-tag-name" data-tags="ç«èµ›">
                <span class="iconfont-archer">&#xe606;</span>
                ç«èµ›
            </span>
            <span class="sidebar-tag-name" data-tags="è£…æœº">
                <span class="iconfont-archer">&#xe606;</span>
                è£…æœº
            </span>
            <span class="sidebar-tag-name" data-tags="pytorch">
                <span class="iconfont-archer">&#xe606;</span>
                pytorch
            </span>
            <span class="sidebar-tag-name" data-tags="svm">
                <span class="iconfont-archer">&#xe606;</span>
                svm
            </span>
    </div>
    <div class="iconfont-archer sidebar-tags-empty">&#xe678;</div>
    <div class="tag-load-fail" style="display: none; color: #ccc; font-size: 0.6rem;">
        ç¼ºå¤±æ¨¡å—ï¼Œè¯·å‚è€ƒä¸»é¢˜æ–‡æ¡£è¿›è¡Œå®‰è£…é…ç½®ï¼šhttps://github.com/fi3ework/hexo-theme-archer#%E5%AE%89%E8%A3%85%E4%B8%BB%E9%A2%98
    </div> 
    <div class="sidebar-tags-list"></div>
</div>

        <div class="sidebar-panel-categories">
    <div class="sidebar-categories-name">
        <span class="sidebar-category-name" data-categories="ç”Ÿæ´»æƒ…æ„Ÿ">
            <span class="iconfont-archer">&#xe60a;</span>
            ç”Ÿæ´»æƒ…æ„Ÿ
        </span>
        <span class="sidebar-category-name" data-categories="çˆ¬å‘æ€»ç»“">
            <span class="iconfont-archer">&#xe60a;</span>
            çˆ¬å‘æ€»ç»“
        </span>
        <span class="sidebar-category-name" data-categories="å¹²è´§">
            <span class="iconfont-archer">&#xe60a;</span>
            å¹²è´§
        </span>
        <span class="sidebar-category-name" data-categories="çŸ¥è¯†æ¡†æ¶">
            <span class="iconfont-archer">&#xe60a;</span>
            çŸ¥è¯†æ¡†æ¶
        </span>
        <span class="sidebar-category-name" data-categories="å­¦ä¹ ç¬”è®°">
            <span class="iconfont-archer">&#xe60a;</span>
            å­¦ä¹ ç¬”è®°
        </span>
        <span class="sidebar-category-name" data-categories="ç”Ÿæ´»æ€»ç»“">
            <span class="iconfont-archer">&#xe60a;</span>
            ç”Ÿæ´»æ€»ç»“
        </span>
        <span class="sidebar-category-name" data-categories="ç¼–ç¨‹ç»éªŒ">
            <span class="iconfont-archer">&#xe60a;</span>
            ç¼–ç¨‹ç»éªŒ
        </span>
        <span class="sidebar-category-name" data-categories="ç¼–ç¨‹">
            <span class="iconfont-archer">&#xe60a;</span>
            ç¼–ç¨‹
        </span>
        <span class="sidebar-category-name" data-categories="ç®—æ³•ç¬”è®°">
            <span class="iconfont-archer">&#xe60a;</span>
            ç®—æ³•ç¬”è®°
        </span>
        <span class="sidebar-category-name" data-categories="ç®—æ³•">
            <span class="iconfont-archer">&#xe60a;</span>
            ç®—æ³•
        </span>
        <span class="sidebar-category-name" data-categories="å­¦ä¹ èµ„æ–™">
            <span class="iconfont-archer">&#xe60a;</span>
            å­¦ä¹ èµ„æ–™
        </span>
        <span class="sidebar-category-name" data-categories="æ•°æ®é›†">
            <span class="iconfont-archer">&#xe60a;</span>
            æ•°æ®é›†
        </span>
        <span class="sidebar-category-name" data-categories="æ€»ç»“">
            <span class="iconfont-archer">&#xe60a;</span>
            æ€»ç»“
        </span>
        <span class="sidebar-category-name" data-categories="æ”¶è—">
            <span class="iconfont-archer">&#xe60a;</span>
            æ”¶è—
        </span>
        <span class="sidebar-category-name" data-categories="æœºå™¨å­¦ä¹ ">
            <span class="iconfont-archer">&#xe60a;</span>
            æœºå™¨å­¦ä¹ 
        </span>
    </div>
    <div class="iconfont-archer sidebar-categories-empty">&#xe678;</div>
    <div class="sidebar-categories-list"></div>
</div>

    </div>
</div>

        <!-- site-meta -->
        <script>
    var siteMetaRoot = "/"
    if (siteMetaRoot === "undefined") {
        siteMetaRoot = '/'
    }
    var siteMeta = {
        url: "https://ailyong.cn",
        root: siteMetaRoot,
        author: "ğŸ†"
    }
</script>

        <!-- import experimental options here -->
        <!-- Custom Font -->

        <!-- main func -->
        <script src="/scripts/main.js"></script>
        <!-- fancybox -->
        <script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.36/dist/fancybox/fancybox.umd.js" onload="window.Fancybox.bind('[data-fancybox]')" defer></script>
        <!-- algolia -->
        <!-- busuanzi -->
            <script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script>
        <!-- async load share.js -->
            <script src="/scripts/share.js" async></script>
        <!-- mermaid -->
            <script src='https://cdn.jsdelivr.net/npm/mermaid@8.11.0/dist/mermaid.min.js' onload="window.mermaid.initialize({theme: 'dark'})" async></script>
    </body>
</html>
